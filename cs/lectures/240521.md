# May 21, 2024 Systems Design Day 2

## HW review

### Data normalization (Team 4)
- What is it?
  - process of re-organizing data to make it easier to work with and query
  - removing unstructured or redundant data
  - data normalization follows rules called "normal forms"
    - how many normal forms?
      - there are typically 3 normal forms, everything beyond 3 is more academic in practice
      - it is unlikely that anyone will ask you about what the rules are in each normal form


- over-normalization:
  - applying too much normalization which creates inefficiences and makes the database cumbersome
    - e.g., too many joins when querying which increases query time
    - normalizing on the wrong field can create more trouble
      - e.g., can't normalize on a field which contains a full address since it is unlikely to have duplication of data here (a 1:1 relationship doesn't require normalization)
      - data normalization makes sense in a 1:many relationship


- pros:
  - data integrity and consistency
    - e.g., eliminating typos (e.g., different ways to spell, indicate the state of California)

  - ease of updates
    - change to one table shouldn't affect other tables
    - there's 1 place to update the data that represents 1 thing (e.g., a state name like California)

  - reduce redundancy
  - efficient storage
    - data is stored in only 1 place


- cons:
  - complexity of data schema
  - slower query execution (use of joins - involves multiple tables to execute a query)
  - slower write operations
  - data analysis is more complex


- may want to denormalize: to speed up queries (prevent use of joins in queries)


- when initially creating a db:'
  - better to overnormalize initially?
  - normalize: take something that exists in 1 table and break it out into 2 or more
    - decision to de-normalize will depend on data usage (e.g., queries)
    - if there are data integrity issues, may need to normalize data
  - when designing the data schema for the first time, there's no "before state"

- normalizing or denormalizing both essentially require the same effort

- note the difference between front-end input validation (eliminating typos by selecting CA from a dropdown)
  - remember that the database may be accessed in different ways - API requests, etc. not just through the front-end UI
  - managing database constraints is the fundamental data-management technique

- names: any need to normalize into first and last name?
  - if you always want first and last, it's best not to normalize since you wouldn't want to retrieve the full name over 2 tables
    - also, there are legitimate different spellings of the same name, so there may not be much value in normalizing


#### Class example - data normalization
- given a table containing students with student attributes:
  - studentID
  - firstName_student
  - lastName_student
  - gpa
  - firstName_advisor
  - lastName_advisor
  - advisorTitle
  - advisorDepartment

- could break this up into 2 different tables:
  - student
    - id
    - firstName
    - lastName
    - gpa
    - advisor_id

  - advisor
    - id
    - firstName
    - lastName
    - title
    - department

- steps:
  - 1. create new schema (incl. constraints)
  - 2. data migration to new tables
  - 3. update application code (account for the new table)


- why denormalize?
  - any bias for faster reads
    - e.g., for archiving
  - 1:many vs 1:1:
    - if data in both tables ends up being 1:1, then it makes sense to de-normalize
    - think about what top-level entities are:
      - e.g., address isn't typically a top-level entity (e.g., for launchschool)
        - but for a real estate listing, or for a post office, an address might be a top-level entity for those applications


- **top-level entity**: any "important" thing for the schema, which is usually it's own table in a data schema


### Indexes (Team 3)
- can think of these as indexes in a book
  - almost exactly the same


- indexes are a separate table created with all the rows, but only selected columns
  - typically these indexes are sorted in a specific way to make a query easier
  - indexes could be implemented as a b-tree or hashes


- why are they useful?
  - leverage the sorted structure to make searching much more efficient
  - it's faster and easier to search a smaller table and then lookup full data from another table if required


- pros:
  - speed up searching / queries by using binary search
    - simplifying something of linear complexity O(N) to log complexity O(logN)


- cons:
  - take up space
  - slow down write/update/insert/delete
  - additional overhead to use and maintain
    - modify queries to leverage indexes
    - manage redundant indexes


- for read-heavy applications, use an index
  - most applications are read-heavy and thus indexes make sense
  - e.g., twitter - much more read vs write (more people will read a post)

  - indexes could speed updates, but not inserts
    - i.e,. searching for an entry first and then updating; the searching would be faster


- indexes are usually in-memory (RAM) to make them faster
  - we usually have more space on disk storage and db's typically write to disk for persistent storage
  - but typically RAM is much faster (although that's changing a bit now)
  - indexes are also written to disk for persistence, but the indexes are loaded into memory


## In-class exercise:  create an ERD
- adding the join table explicitly:
  - it makes more sense to show the join table if there are additional attributes to add to that table
    - e.g., who is the owner, when was the relationship added, etc.

- when including a join table in the ERD diagram - will need to follow-up to see if the cardinality is based on the primary entities rather than the join table
  - it probably only makes sense to include the join table if there are additional attributes that are required
  - it's also better to state assumptions in an interview
    - e.g., I will break out this many to many relationship into an explicit table

- for ERDs: typically just looking at "nouns"
  - for OOP, may think about "verbs" and "nouns"
  - note that the creation of the todo is distinct from the data that we might store about that todo
    - hence, a link between user and the todo may not strictly be necessary if it's not directly stated in the use cases

- creating use cases are helpful to frame the solution - check with the interviewer
  - they make add a use case later, etc. and the diagram may need to be updated to accommodate

- think about "search" functionality as "filtering"
  - i.e., among the list of all possible data, we'd look for data associated with some specific attribute (e.g., users, etc.)


## Self-referential relationships
- e.g., think of a table "employee"
  - attributes:
    - id
    - name
    - department
    - salary
    - title
    - assets
    - date_of_hire
    - manager_id

- manager_id:
  - a little bit unique since the manager is ALSO an employee
  - this is a 1:1 self-referential relationship
  - you COULD break this out into a different table if:
    - there are other attributes that are specific to managers
    - could also just add (optional) columns within the existing employee table that are for managers
  - manager_id may also be optional (e.g., the CEO may not have a "manager")
  - a manager has 1 or more managers
  - an employee has 1 or no managers


- e.g. book recommendations
  - may have a "books" table:
    - id, title, author, publisher, genre
  - would be best to add a "recommendations" table
    - id PK, recommender_id FK, recommendee_id FK
    - this creates a many to many relationship between books

  - shouldn't be using an array type within relational dbs
    - if you have a "compound" value, you should probably create a table and use joins to manage that 1 to many relationship


## Shopping cart exercise
- build a shopping cart
- top entities:
  - cart (essentially 1:1 with user, so don't need user as a top entity)
  - products

- can build a "product" table
  - id PK
  - SKU
  - name
  - price
  - inventory

- can build a "cart" table
  - id PK
  - intuitively we may want to add, but actually shouldn't:
    - product_id FK
    - quantity

- need to think about implicit entities
  - need a join table to store items that you've added to your cart
  - need to figure out what each record in the table represents
  - hence, can't just add product_id FK, quantity to the "cart" table

- hence:  create a "cart_item" table
  - id PK
  - cart_id FK
  - product_id FK
  - quantity (amount added to cart)
  - price (this is repeated since price of the product may change later, but you'd want to track the price at which the product will be purchased)
  - order_id
  - order_date

- price : within the "cart_item" table:
  - this needs to be included, and provides flexibility to accommodate different business logic
  - e.g., should the price change in the cart if the "real-time" prices changes before you checkout?
    - business logic (requirements) will need to be determined and addressed within the app


- NOTE:  it's best practice that primary keys are DISTINCT from any other piece of info within the database
  - it's best for security that the PK is used only for the db
  - it's best not to expose your database to the rest of the world


- "soft-delete" : means there might be some additional attributes like "deleted?" or "date_deleted" to track whether or not that item was later removed


- Discounts:
  - could have per-product discounts
    - there may be a code for a specific product
    - there may be BROAD discounts
  - could also have per-cart discounts
  - could create another "discount" table
    - include id PK, discount_type, discount_amount, start_date, end_date
  - might then apply the discount to the cart_item OR product OR cart (as appropriate for discount type)
    - where you reference the discount will have a different effect on the scope


## Interviews
- typically the problem will be something like "design reddit", "design twitter", "design a shopping cart", etc., 
  - could also be "design a notification service", "design a messaging service"
- need to be able to reason through things
- what might the architecture of a chat room system look like?
- what might the system architecture be to allow users to chat in real-time
  - how could you scale the system?
  - what if you had multiple chat rooms?
  - could you be in multiple chat rooms at once?

- typically, there's no objectively correct answer, but they want to see that you're thoughtful about what you consider
  - do you make decisions backed by reason
  - be confident and positive - think soft skills
  - be someone that others want to work with

- interviews are typically driven by the interviewer - could focus on specific parts, could push the conversation in different ways
  - follow their lead - don't try and push back
  - interviewers may push you beyond the initial prompt (e.g., change constraints, etc.)
    - they're looking for boundaries of your capability
  - saying "I don't know" isn't necessarily a bad thing - be clear about what you know and what you don't know
  - you could always say, "I'd guess", or "my approach would be..."

  - requirements will always change when you're working as a software engineer


- within "system design" there is a subset of "functional system design"
  - typically focused on current state (i.e., today)
  - for functional system design, it's typically to the ERD level and possibility talk about schema
  - the ERD represents the current state
  - some interviews may ask for sql statements



- after ERD, will typically ask about scaling questions
  - then this becomes more of a infrastructure question


- if you're not familiar with the app they're asking you to create, you should indicate that
  - you could also ask for a different scenario

- "I'm going to make assumptions..."
- ask if interviewers are okay with any decisions you make
  - they typically have an end goal in mind, so check in with them to see if you're heading in the right direction

- consider the amount of time available
  - if you've only got 30 mins, better to have fewer use caess
  - if you've got 90 mins, it might be better to have more use cases

- interview could be on the phone:
  - you'd have to jot things down for yourself
  - typically there will be a whiteboard (miro, excalidraw, etc.) so you can diagram
    - they'll typically provide a login

- when making ERDs, build them up gradually
  - think about it use case by use case

- if asked to create SQL statements
  - you could indicate that a requested query might be very cumbersome or difficult
    - worth having a discussion about that
  
- for data attributes:
  - think about how exact that metric needs to be
  - is it a "cached" value?
    - something that exists upstream to shorten the retrieval time
  - calculated vs real value?
    - a cache value may also refer to a value that isn't the source of truth (it's derived from something else) 
    - may also refer to redundant data used to speed performance
    - the calculated value would be "derived" and NOT the source of truth
    - the "real" value would be the "source of truth"

- roles in dev (from the bottom - most junior):
  - this refers to the "IC" route:  individual contributor
  - the problems that are worked on become larger as you get more senior, problems get more ambiguous


  - junior / associate engineer : typically get well-scoped tasks
  - software engineer (SWE II roles) : might get a problem and have to figure out
  - senior engineers : here's a difficulty, figure out the problem and the solution
  - staff engineers : typically takes 5 minimum to reach staff
  - principal engineers


  - mgmt roles:
    - Team lead
    - Engineering Mgr
    - Director of Eng
    - CTO