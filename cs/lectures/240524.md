# May 24, 2024 lectures

## Initial discussion on general system design numbers
- it's generally faster to filter and sort in the DB than in code
  - i.e., don't pull ALL data and then filter in app code
    - better to just use the right sql query
- cache is must faster

- for SQL db
  - it's a data volume problem - opening (and closing) a connection is very expensive - takes time
  - hence connection pooling - reduces the need to open and close the db which speeds up the response
  - if all workers are busy in connection pool, then requests might get dropped and require a 2 - 5 second timeout

- don't mix up bytes and bits
  - 1 byte = 8 bits

- document stores are better at retrieval of data in certain formats
- relational dbs tend to be better at slicing data

## Caches
- see notes on LRU Cache
- LRU is very common - a kind of eviction strategy for cache
  - Least Recently Used
- may need to code up a simple LRU cache live as an implementation
- often implemented using a doubly linked list with a hash (common for cache in general, not just LRU cache)
  - typically both hash and cache (doubly linked list) are collectively referred to as the cache


- LRU / MRU :  least / most recently used
  - the order in which data is accessed
- LFU: least frequently used
  - how often data is accessed
- LIFO : last in first out (stack-like)
- FIFO : first in first out (queue-like)
  - the order in which data is added to the core
- random replacement:
  - just randomly remove stuff from the cache


- MRU - examples:
  - eject the most recently accessed item from the cache
  - e.g., Netflix - most recently watched TV episode can be ejected from cache
  - e.g., Tinder - someone you've swiped no can be ejected from cache


- LFU - least frequently used
  - when the frequency is directly proportional to the usefulness
  - e.g., web server cache for frequently accessed files


- LIFO:
  - like a stack
  - e.g., browser history - clicking back pops the item off the back of the stack


- FIFO:
  - like a queue
  - e.g., checkout line at grocery store
    - eject item that is in the queue the longest
  - e.g., message notifications
  - e.g., versioned files, mostly likely to evict the oldest version
  - e.g., quick access toolbar - you might get rid of the tool that was used more previously


## CAP Theorem
- see notes on basecamp

- a consistent and available system has only 1 node
- once you partition into multiple nodes:
  - you can choose sync replication or async replication

- a partition:  something breaks the network and nodes can no longer communicate

- once there is a partition you get a choice:
  - choose availability or consistency

  - e.g., ATM network connection breaks:
    - choose consistency:  you can't use the ATM
    - choose availability: let people use the ATM, but the transactions won't be replicated among the rest of the network

- good to have examples:
  - like consistency:  banking (sync with transaction guarantees)
  - like availability:  social media, google docs

- what makes a system available or unavailable?
  - need to consider
- what is availability?
  - it's not always binary (i.e., available or unavailable)
  - how do we deal with slow?
  - Martin Kleppman proposes that availability should be modelled on latency - i.e,. ability to meet requests within some time limit

- partitions:
  - what's a break vs slow?

- linearizability:
  - probably not important for interviews
  - but consistency means different things in different contexts


## ACID / BASE
- see ACID / BASE notes in basecamp
- transactions:
  - a sequence of 1 or more sql statements that are all required

- Atomic:
  - transactions are all or nothing
  - if things take time, you guarantee there's no halfway states
- Consistency:
  - only valid data is saved : if there is a constraint, must abide by it
- Isolation:
  - 1 transaction can't read data from another transaction that hasn't happened yet
    - isolation levels:
      - serializable
      - repeatable read, snapshot isolation
      - read committed
      - read uncommitted
- Durable:
  - transactions, once complete, info can't be lost

- transaction guarantees are only available on a single machine
  - hence it makes sense to prioritize vertical scaling on (relational) DBs
  - if sharding, harder to get transactional guarantees



- PostgreSQL : you can define TRANSACTIONS using a particular function to group a set of sql statements together
  - the example used in the postgres documentation is a banking example (e.g., need to transfer money to someone else)


- associate CP with relational databases

- associate BASE with AP (weak consistency)
  - generally refers to NoSQL dbs
  - e.g., BigTable / HBase, DynamoDB, Cassandra
    - all have improved write performance
    - since there are no guarantees, performance is improved over ACID


- NoSQL dbs
  - flexible schema but inflexible queries
  - tends to be better for social media, likes, etc. - anything that is write-heavy
  - e.g., early twitter service that could track trends, would hit twitter API to get data and recombine data
    - Twitter API was changing a lot since schema was changing
    - so using NoSQL db was a much better choice since the twitter API schema kept changing
  - if you don't have a strong schema, a NoSQL db is definitely the right choice


- relational dbs
  - inflexible schema, but flexible queries
  - these kinds of dbs are much more mature : have more tooling and abstractions available
  - adding more memory to scale these is a great technique - they use cache



- for interviews:
  - better to start by modelling with relational DBs
  - when the discussion about scaling starts to occur, then you could bring up NoSQL dbs as a tool to improve performance as you scale (e.g., write heavy, flexible schema, etc.)


## Webhooks
- it's like a push notification for some kind of web event
  - the vendor sends that information
- e.g., for github:
  - to get notifications from github on pushes, commits, etc.
  - you need to give github a URL so that they can send a message when something happens
  - github could use an email instead, but it's hard to CHAIN activities to occur based on that event
- a service must create a webhook service - they would define the events available
  - you as a user would have to "subscribe" to those webhooks

- webhooks is a coding pattern and also an feature of websites
  - e.g., github provides a list of webhooks
  - e.g., basecamp provides different webhooks

- also need to know the schema of a request sent by the provider
  - you can figure this out using requestBin
  - requestBin gives you a URL to send requests to
  - if there is no documentation (and you didn't use requestBin)
    - you'd have to host your own endpoint to receive webhooks
    - then do a console.log to see the request and try and parse it
    - the disadvantage to a console.log it's not persistent, although there may be log files somewhere
      - the log for a production server is probably busy - a lot of things are happening constantly

- "tailing the log":
  - a term that means watching the constant stream of info in a console.log scroll by
  - comes from an old unix term where there was a tool called tail

- why did github setup a webhook?
  - to prevent people from constantly hitting the API for information ("polling")
    - typically "polling" (repeatedly asking for info) vs "long-polling" (leaving request-response cycle open: you ask is it open? and then leave the request open until there's a response or a time-out)
  - typically polling is done with a cron job

- webhooks are a form of event-driven architecture
  - you wait for an event to occur before something else happens
    - this is a "push" paradigm
  - APIs are typically a "pull" model
  - with webhooks, you can use the event to drive some subsequent event

- whats the response for a webhook request?
  - respond to github with some positive confirmation of receipt
    - 200 status code is typical
  - github: if you don't return a 200, they'll let you know that something is wrong
  - most services don't care - they just keep "shouting into the void"

- there are a lot of applications for webhooks
  - example LS lesson content:
    - lessons are created as markdown files and placed in particular folders
    - there are content repos on github, where each repo is associated with a particular course
    - then the repo is updated, a webhook is triggered and sent to the app to indicate that updates have been made
      - the actual content is not sent, but a lot of metadata is sent
      - but all info required to get that information is provided
    - the app then sends a response (e.g., 200)
    - the LS app then sends a request for that information
    - github then sends the actual content which the LS app can use
    - LS then updates their own DB with the information so it can persist

  - the actual source of truth is actually github
    - the LS app db is like a cache

  - troubleshooting if a change is made to github and it's not updated on the website
    - would first check github
    - then check the db
    - if it's not in the db
    - then would need to check production logs to look for the webhook and see how it was received (and if it changed, etc.)
      - using requestBin allows you to quickly see what the webhooks sent were
    
    - some services providers don't allow you to send webhooks to more than 1 endpoint
    - an additional functionality could be to forward on webhooks to additional endpoints

    - for our project, we'll focus on just collecting and inspecting the webhooks (not forwarding)

- webhooks tend to be POST requests so that they can send information within a body
  - you could also send GET requests
  - there might be standards, but not everyone might abide by those standards
  - github has one of the best webhook implementations
    - most other implementations are just "best efforts" - they don't spend extra time since it's not a core feature

- some considerations:
  - ordering?  what is the order of webhooks?
  - how do you ensure 2 events are processed in order?
  - what is the order?
  - if webhooks retry - how do you know if a hook was previously successful?

  - idempotency: 
    - saying "add 2" vs "set it as 5"
    - if you repeat "set it as 5" : an "idempotent" action
    - repeating "add 2" will lead to different results

  - how to deal with DDoS?
  - is destination URL legitimate?
  - it's possible to create an infinite loop of webhooks
  - what about timeouts?
  - what about processing queues?
    - if we receive a webhook and add a processing event to a queue, what happens if there's a fault in the queue?


## RequestBin:
- see requestBin requirements in basecamp

- create 1 requestBin per team
- we work on this until next Friday
- will do (fun / light) demos on Friday
  - have fun with the project

- we need to create 2 urls:
  - one to send things to
  - one to view the bin at

- optional requirements:
  - users associated with specific bins
  - real-time functionality:
    - can do that with websockets or service ...
  - how to clean-up bins?
  - use React as practice
  - copy button, send sample requests


## ngrok
- why use ngrok?
  - our laptop doesn't have a publc IP
  - ngrok allows you to access a localhost from the wider internet
  - github cannot send you a webhook "out-of-the-box"
    - to build something locally to receive a webhook, ngrok will let you expose that 

- why not SSH into VPS and code on VPS?
  - think of the VPS as a production environment - don't develop in your production environment
  - theoretically, you could just use VSCode and edit remote files, etc.
  - however, it's bad practice to dev in production
  - best to do dev and then when everything is stable for users, then deploy (copy code to production)

- ngrok is "public" and allows you to access your localhost application from the wider internet
  - it gives you a public URL that can be used for webhooks
  - ngrok is a dev only thing for us at this point


## Teamwork
- can use Live Share within VSCode - should be able to be in the same session and collaborate
- there are different options for teamwork
  - could have 1 person using ngrok to test and everyone else shares
  - should be able to work on different files in Live Share

- could also create a dev environment VPS and code through that
  - this is a less common pattern to Nick

- document!
  - it's best to keep notes as we go
  - writing things down is a great reference
  - write what you've done and what you've researched
  - need to have a trail of breadcrumbs
    - need to talk about the "journey" and interesting problems solved


  - writing for research is helpful as a sign of progress
  - research may be about closing doors, rather than opening them
    - so keeping a log of interesting things
    - a log of research path
    - questions raised and answered
    - it helps to gauge progress

  - stuff you research now may be more useful later (e.g., in interviews, etc.)



## RequestBin ERD
- looked at Team 4 ERD
- "log" has a very specific meaning in development
  - avoid calling things "log" that aren't strictly a log
- remember to keep unique_ids for tables separate from public randomly generated unique identifiers for each bin
- may need timestamps for when webhooks are received

- could include a session id when accessing the site to keep track of users and knowledge of the bins that users have created

- could parse out method, path, etc.
  - if so, always save the raw request payload
  - generally, for any data processing, always keep the original in case something happens that isn't appropriate


- timestamps:  what kind of value is it?
  - a string: typically in UTC time
  - time is very tricky for software engineering
  - it's best to always store a UTC timezone and then translate that into each users preference when displaying
  - or can store epic time - number of seconds elapsed since some date


## Federated Databases
- when companies get too large and there are too many databases
- a layer of abstraction is created on top of the database layer
  - you may have many different databases
  - sometimes you create an abstracted internal API that your internal app servers will query to pull data from the specific dbs from the right APIs


## General Numbers for System Design

Network Latency:

US Coast to Coast (one way): ~ 100 ms
Trans-Atlantic: ~ 80ms
Trans-Pacific: ~ 150ms

Web App Performance

Cache query: 1s of ms
SQL database query: 10s of ms
HTTP Request/Response: 100s of ms (including SQL DB queries. below 300-500 is generally considered ok)
Complete page rendering: multiple seconds, including fetching all assets, parsing HTML, CSS, and JS, and painting page

Database Queries Handled Per Second

    SQL DB
        PostgreSQL / MySQL
        ~ 1K QPS
    HDD NoSQL
        MongoDB
        lots more options here
        ~ 10K QPS
    Memory NoSQL
        Memcached / Redis 
        global hash table
        100K - 1M QPS


A Dedicated Low-End Commodity Server

Memory Size: 16 GB
Disk Size: 1TB

Bits and Bytes

1 byte = 8 bits

    8 bits: maximum representable value 2^8 − 1 = 255
    16 bits: maximum representable value 2^16 − 1 = 65,535
    32 bits: maximum representable value 2^32 − 1 = 4,294,967,295
    64 bits: maximum representable value 2^64 − 1 = 18,446,744,073,709,551,615
    128 bits: maximum representable value 2^128 − 1 = 340,282,366,920,938,463,463,374,607,431,768,211,455





## LRU Cache Notes (from basecamp)


A cache is a hardware or software component that stores data so that future requests for that data can be served faster. Caches eventually get full, so there are a variety of cache replacement algorithms available to determine what data gets removed from the cache. LRU (least recently used) is a cache replacement algorithm that evicts the least recently used entry from the cache to make room for a new piece of data.

A LRU cache can be implemented using a hash and a doubly linked list (functioning as a queue). Using these data structures, the cache is very performant with O(1) access, insert, and delete functionality. The downside is that it uses O(n) space, with the linked list of size n and hash holding n items.

The hash holds items as keys with the corresponding node address as the value. When the data is accessed, the reference is used to retrieve the node, which holds the wanted data.

Zoom image(13).png
Hash and double linked list 24.8 KB View full-size Download

In terms of operations, every time an item is accessed:

    Look up the item in the hash

    If the item is in the hash, then it's in the cache

    Use the hash to find the corresponding node
    Move the node to the head of the linked list


  3. If the item isn't in the hash, it needs to be loaded into the cache

    Is the cache full? If so, remove the node at the tail of the linked list (LRU) and the item from the hash.
    Create a new linked list node for the item and insert it at the head of the linked list.
    Add the item to the hash, storing the new linked list node as the value.


More reading:

    [ConvoStore](https://corvostore.github.io/)
    [Interview Cake - LRU Cache](https://www.interviewcake.com/concept/java/lru-cache)
    [Geeks for Geeks - LRU Cache Implementation](https://www.geeksforgeeks.org/lru-cache-implementation/)
    [Sunny's article on building an LRU cache](https://betterprogramming.pub/how-to-build-an-lru-cache-in-less-than-10-minutes-and-100-lines-of-code-fddad56d7af5)



