# Monday May 27, 2024 (Week 4 Day 1)

## Cron jobs
- jobs that run at a fixed interval
- typically a script
- they're run as background processes
- these jobs are VERY common
- some jobs are event-based jobs (e.g., like webhooks)
- other things might be time-based (e.g., cached counts, etc.)

- see https://cron.help
  - look for cron syntax, cron translater
  - will help to confirm the syntax to make sure things run when you think they'll run

## Example
- think 3 tier architecture (our backend)
- a user (from the public internet)

- 1. when a user first signs up, they fill in a form and make a request
    - doing this to combat spam and bots (prevent programmatic use of the service)
  - when the request comes in from the user:
    - the webserver will proxy the request to the app server
    - the app server will route the request and process it and then submit a record to the database
      - 2. likely a record in a 'user' database; it makes sense to create a 'status' field to track the user status
        - the more specific the status, the easier it is to keep track of what steps (and what might break) within an overall user workflow
        - e.g., status: `pending_email_outbound` : be as granular as possible for your future self
          - the user may never receive the email and you may need to debug the workflow
  - sending email is a "slow" thing
    - hence it makes sense to do it as a background job and not do it synchronously
    - may want to validate the email address on the backend
    - goal is almost always to speed responses to the user:  want to send a response to the user ASAP
    - sending emails could be multiple seconds
    - waiting for the email would potentially be a "blocking" action
  - need to "dump" the email job somewhere so that it can be tracked and kicked off
    - 3. add the email job to a queue
      - what we add to the queue depends on the responsibility of the queue
      - if there are many different jobs in the queue, then the queue message will need to be more specific (i.e., the queue is only for activation emails vs a queue for ANY kind of email, payments, etc.)
      - LS would typically have 1 queue for ALL email jobs
        - thus, the entire email would need to be in the queue message ("data package")
      - to (user email)
      - from
      - subject
      - body
      - if timing was critical (e.g., specific emails for certain time)
        - that logic should be in the app server or a cron job
        - only emails that need to be sent should go into the queue
      - need to turn all of that data (typically and object) into a string (e.g., JSON) that can be transmitted to the queue
        - "serialize" : means turning it into a string
        - "de-serialize" : means turn it back into an object
  - 4. send a response to the user
    - could indicate if you don't see the email, click a link, etc.


- queues are used to decouple parts of your system
  - creating an email queue decouples the sending of email from sending a response to your user
  - all of the activities downstream of sending an email are separate from sending a response to the user
  - queue can also change the velocity of the work - rate at which jobs are added can be separate from the rate at which jobs are fulfilled
    - also called "flattening the curve" - reduces the curve (load) of traffic

- 5. need to dequeue job from the queue and fulfill it
  - could use a python script on it's own node to dequeue the job
  - typically a "worker" cron job would pull the job from the queue and do the job
    - this is typically on a time-basis
  - cron jobs are typically much easier to setup to manage the queue rather than create a pub/sub queue (see basecamp notes on message queues)
  - if there are messages in the queue, the python script will execute the job
    - formats need to be determined beforehand so that the python script can execute

- 6. the actual email will be sent via an smtp server
      - emails can be tricky, if you mess it up, you can get blacklisted
        - most people use a transactional email provided (e.g., mailchimp, sendgrid, mailgun)
      - typically accessed via API call to that provider
        - the provider will send an "optimistic" response - it just confirms that the call was received
        - typically, it's up to you to check later to see if things were sent
        - we typically assume it was sent correctly

- 7. python script needs to update the status within our user db:
  - e.g., update status to : `email_request_to_mailchimp`
  - status needs to be specific enough to support debugging later


- sometimes there is a DLQ: Dead Letter Queue
  - if something happens to a job in the email queue (e.g., a job is malformed and can't be processed by the Python script)
    - that job is placed in the DLQ
    - something may be setup to provide slack channel notifications to indicate there was a failed job


- see basecamp notes on message queues:

  - there are 2 different types of queues:
    - i) point to point : they just hold things that are "in-flight"
      - e.g., wait staff take orders and put them in a queue for chefs to make
      - typically FIFO
      - each of the messages are singular, relatively undifferentiated

    - ii) pub/sub queue : more like magazine subscriptions
      - when there's a new issue then you get pushed a new issue
      - here the messages are different - specific messages are pushed to the subscribers that want those messages
      - kafka is the most popular framework for pub/sub messages
        - i.e, provider/consumer libraries take care of the messaging

- discussion on managing the "status" within the db
  - decision to normalize vs denormalize that status (i.e., create a separate data table with all of the different text status codes)
  - in a small system, you could just manage the status as a string directly in a db field
    - it's only used internally, potential for a typo is relatively low (it's not taking user input)
  - however, we could denormalize those statuses:
    - query speed is less of an issue since it would typically only be accessed for troubleshooting or analytics
    - if the script and the user workflow were managed by different teams (i.e, an "organizational" reason) it may make sense to denormalize - will help communicate an interface between the 2 teams


- component diagram
  - doesn't have anything to do with scaling
  - it just demonstrates a current state and outlines the current structure
    - there may be 5 instances of the python script, but it doesn't need to be shown


- if using OAuth for authentication:
  - won't have any authentication info, but will still need to have a record in the db for that user (there will be app-specific info that needs to be saved)
  - https://www.linkedin.com/posts/alexxubyte_oauth-20-explained-with-simple-terms-activity-7167560207580016640-AoCu/




## Authentication
- there is usually an authentication separate from a user table
  - authentication tables are used frequently (every login), but user tables are used relatively infrequently (don't need other info like birthday, etc.)
  - hence, authentication tables are relatively small vs the user table

- hash functions:
  - these only work 1 way: will change the password to make it look different, and there's no way to take the output of the hash function to derive the password
- salt:
  - need to add some string which is appended to the password which ensures that the hashed value can't be stored in a table and looked up again


## Ticketmaster in-class exercise

- https://learningdaily.dev/when-taylor-swift-crashed-ticketmaster-a-lesson-on-scaling-for-spikes-9931e2c888e9

- need to start with a current state for metrics
  - then can talk about how much to scale for peak load

- CDN:
  - these generally don't sit IN the back-end
  - typically, these would be part of the wider internet (global network of servers to deliver content)
  - could be seen to be part of the wider internet
- think about difference in content between caches:
  - e.g., from another team's diagram, the read-thru cache might have had the same content as CDN
  - best to think of specific examples of info that might belong in the read-thru cache

- need to think about unexpected surges and surges you can prepare for
  - ticketmaster SHOULD have known there would be a surge - they released the tickets
  - they could have spun up additional instances, could have pre-populated caches, etc.
  - this wasn't an unexpected surge

  - placing people in a waiting list queue is helpful
    - could be up on the webserver to re-direct to the waiting list queue
    - e.g., see 

- think about queues - as a way to facilitate an async process
  - in our component diagram, need to think about what the user is seeing
  - we could have thought more about what the queue is specifically doing and where it sits
    - i.e,. could be for db writes
    - or could be for payment processing management

- in Nick's diagram
  - used cache-aside, since there is a backup path through the message queue and load balancer
    - since cache-aside isn't a bottleneck and SPOF, it can be better to use cache-aside and not read-through cache
  - the queue is used to protect the db
    - the worker also gets to respond to users right away and not worry about the payment processor


## Homework
- be comfortable with 3-tier to N-tier scaling path
  - practice this with our team from 3-tier to database replication
  - add app servers, make them stateless
  - add read replica dbs and load balancers in front of them

- consistent hashing (can be difficult)
  - make sure we understand
- some other readings on git commits, etc. with multiple authors

- requestBin project:
  - don't focus on 1 piece at a time
  - create the crudest working version first and then build up
  - just build anything that can do something:
    - i.e., v1 is backend that accepts requests
    - v2 throws the request in mongo
    - v3 displays text on-screen
    
    - keep working on minimum viable products in stages



### (basecamp)

Group Work:

3-tier to n-tier Architecture - Optional

This exercise is optional because most everyone should be comfortable with this topic by this point, but it's listed first because it is the most important. This is the foundation upon which week 4 is built, and everything will make more sense if you understand this. If you are not 100% comfortable with these concepts, please do not skip this exercise. Leverage your team to help explain concepts you are not clear on 

    Write down the progression from 3-tier architecture through db replication. 
    Pretend these are speaking notes for a system design interview. 
    Do this if you don't feel solid on these concepts.



Homework, Reading, Videos:
The first 4 items are pretty short and should go quick. These concepts are tricky to understand, so there are a couple of resources here on both database sharding and consistent hashing. If you feel like you have a good grasp of it, though, no need to keep digging further. The last item (NoSQL video) is separate and is just short of an hour long, so plan accordingly.

    Watch this video on database sharding: https://www.youtube.com/watch?v=5faMjKuB9bc
    Here's a reading on database sharding: https://www.digitalocean.com/community/tutorials/understanding-database-sharding
    Watch this video for consistent hashing, an answer to the downside of "key based sharding" mentioned in the DO article above: https://www.youtube.com/watch?v=tHEyzVbl4bg
        As you watch this video and the next, think specifically about how consistent hashing reduces the amount of data that needs to be moved when a node joins or leaves the cluster, compared to hashing using a modulo.
        Consistent hashing has more applications than just on the data layer. For example, this video talks about it as a method for load balancing. The principle is the same if you replace "server" with "database node" (as in the below video).
    Here's another video on consistent hashing, applying it specifically to data partitioning: https://www.youtube.com/watch?v=UF9Iqmg94tk
        Two videos because consistent hashing can be hard to grasp when you first encounter it. If it's still not clicking, try [this article](https://www.toptal.com/big-data/consistent-hashing)
    Watch this [Introduction to NoSQL Databases](https://www.youtube.com/watch?v=qI_g07C_Q5I) by Martin Fowler. When watching this video, write down notes about:
        How NoSQL differs from relational databases
        How the different type of NoSQL databases work (document, key-value, etc)
        Use cases for NoSQL databases (why would someone use them)?
        Shortcomings of NoSQL databases
        Any questions you have


Build RequestBin and deploy to VPS! 

    Here's [some information](https://docs.github.com/en/pull-requests/committing-changes-to-your-project/creating-and-editing-commits/creating-a-commit-with-multiple-authors) about collaborating on Github and displaying commits as having been made with multiple collaborators.

