# Database performance, replication, partitioning


## Performance vs Scalability

This document focuses on the scaling path of a database, starting with improving performance and going through database replication and partitioning. 

Performance is different than scaling, however improving performance can significantly change scalability of a system. For example, if add an index to improve a table look up to O(Log(N)) time from O(N) time, this has a big impact on what we need to do to scale the system. 

## Performance

    [Use the Index, Luke!](http://use-the-index-luke.com/)
        Adding indexes is a development task, not a sysadmin task. Adding indexes is to prescribe data's access path/pattern to improve performance.
            Typically self-balancing b-tree ( a generalization of BST)
            Another option is hash
        Adding indexes is to tradeoff
            Writes for reads
                Every write also requires updating the index
            Space for time
                Indexes cost space
        Run EXPLAIN / EXPLAIN ANALYZE 
            Execution Plan
            Never do full table scan (*). 
                If you see "Seq Scan", add an index!
        SQL keywords that may point to opportunities for indexing
            WHERE
            ORDER BY
            JOIN
            GROUP
    N+1 Issues
        When you need to retrieve a set of related records, but instead of performing one query to get data, the system performs one query to get a main set of records and then perform another query per record fetched
        Often due to how an ORM interprets a database query
        Example - instead of one query to get all books from all authors, a query is made to get all authors, and then a query per author to get each of their books

-- Performant Query, get all records

SELECT a.id, a.name, b.title
FROM authors a
LEFT JOIN books b ON a.id = b.author_id;


-- N+1 Problem: Get all authors
SELECT id, name FROM authors;

-- Then get execute another query per author to get their books
SELECT title FROM books WHERE author_id = ?;



## Scaling a Database

    Adding indexes, while keeping an eye on read/write ratio
    Denormalization
        Eliminate joins by introducing redundant data
        Will mutate database schema and requires data migration strategy
        Often not realistic for large or mature database
    Try to keep indexes in memory
        Database is too large to keep in memory, but keep indexes in memory for speed
        Most relational databases will do this for you
    Analyze data growth velocity
        If growth starts to outpace memory/HDD, then you need a sharding strategy (see Partitioning section below)


## Replication (Primary - Replicas)
### Why need replicas? Why not just 1 database? 

    When we have multiple web/app servers talking to 1 db server, it overwhelms the db. 
        Having replicas allows us to direct reads to the replicas.
    When are users are geographically far away from our database, it takes longer to return a response to them
        Having replicas in different regions can help serve clients faster
    When you only have one database, you have a single point of failure
        If your primary goes offline, the system can still be read-only until one of the replicas is promoted to be the Primary

### What does this look like?

    Write only to Primary (the "system of record")
    Read from replicas 
    Spread QPS across multiple database servers, especially good for read heavy apps

### How does replication work here?

    Asynchronous replication - primary's writes are re-played (federated) on replicas via a "write log"
    It comes with downsides
        Durability - what happens if the primary that receives and confirms a write request fails before it can propagate that change to the replicas?
        Replication lag - what happens if we try to read some data that hasn't been fully replicated yet?
            Certain reads can be served from the primary - i.e. "read your writes"


## Replication (Primary - Primary)
### Why would I need more than one primary?

    This may be necessary when writing to 1 db overwhelms the db server.
    Latency on writes for far away users still a problem
    Single point of failure - is there a process to promote a replica to become primary?

### What does this look like?

    Both (all) databases are writable
    Good for write-heavy apps, or apps with users in distant locations
    Writes may result in conflicts, which may have to be resolved, introducing latency

### How does replication work here?

    Asynchronous replication - data is synced between the primary databases
    This also comes with downsides, namely conflicts due to replication lag - what happens when multiple primaries accept conflicting writes?
        Avoid conflicts in the first place - for situations where you can always accept writes for a specific set of data in a specific database
        Last Write Wins - timestamps are attached to each write, and most recent one is accepted (reliable and synchronized clocks still a problem)
        Allow users to resolve - store conflicting writes and return them all to users, asking them to resolve
        CRDT (conflict resolution data types) - often used in real-time applications (and Capstone projects!)
        Write (and read) from multiple database instances, settling differences with a quorum
        Custom solutions


## Partitioning (Sharding)
### Why partition my data? 

    One big problem - we have too much data. This leads to performance degradation, with side effects like increasing difficulty of managing large indexes in memory 

### What does this look like?

    While each replica holds an entire copy of the data, partitioning involves splitting the data up, like volumes of an encyclopedia.
    Benefits include
        Smaller result sets
        Indexes built faster (faster writes)
        Queries become faster
        Database cache-ability
    Drawbacks include
        Complexity of implementation - application needs to become aware of where data is stored
        Queries across shards become more complex and lose ACID guarantees
        Difficult to find the right partition key - potential for uneven data and load distribution

### How does partitioning work here?
Zoom horizontal_vs_vertical_partitioning.jpeg
credit - @alexxubyte 122 KB View full-size Download

    Vertical Partitioning
        Schema changes to split out columns to a separate table / db
        Similar to normalization, but for a different purpose (modeling vs performance)
            normalization is domain modeling, vertical partitioning is for performance optimization
        Split out large, infrequent used columns so frequently used columns can be cached more aggressively
            For example: split off username and password from Users table for authentication purposes
            Fun detour - [How to store passwords safely in a database](https://twitter.com/alexxubyte/status/1522242694004674560)
 
    Horizontal Partitioning (Sharding)
        Normally when we talk about partitioning, we're talking about horizontal partitioning, or sharding.
        Partitioning should ideally only happen once if the proper key is chosen since it's a decision that is extremely hard to reverse.
        Partition key (or sharding key) is specific to an applications data access patterns. A key should be selected to minimize "hot spots" and cross-shard querying
            For example, common keys could include: 
                user ID
                hash 
                region
                date range or timestamp
        What happens when your shards get too big? You guessed wrong and data grew too rapidly. Shard again.
        Warning: sharding is NEVER a suggestion to be thrown around lightly. It's the very last resort since it's almost impossible to reverse and comes with a substantial increase in complexity.


## Twitter in-class exercise
- sharding:
  - each shard will / can have it's own set of read replicas
  - want to avoid hotspots (read and write)
  - twitter - will probably be more reads

- with sharded databases:
  - don't forget that we need to add a data layer

- once you move to multiple app servers, you'll need to move the webserver to it's own node

- Nick sharded on user ID
  - included cache cluster for recently active users (pre-populated timelines), used LRU for eviction policy

- when sharding:
  - should think about space constraints, hotspots, and cross-shard queries
  - ideally, you can find a natural partition key that can provide all of the benefits
  - the exercise is really thinking about what the different options might be and what the pros/cons of each choice might be

- for interviews:
  - need to be aware of the potential weak-points in any decisions / choices



## Hash functions (Consistent hashing)
- just a function that takes any input and produce an output in a fixed range and the same string gives the same result

- hash functions are often used to "scramble" input

- most basic hash function is "modulo":
  - use modulo to determine which of 4 app servers to send a particular request to
  - could also use modulo to determine which database shard to assign data to (e.g., could has the unique_id from the db)

- why pick using a hash instead of using round robin:
  - round robin would require some concept of state
  - but if you use the hash function, don't need to worry about state

- if the number of servers / shards changes, then the hash function will re-direct to different instances
  - for app servers, this isn't so bad
  - for database shards, it doesn't work, you can't point to a different shard every time (you'd have to move all of your data)
  - don't want to have to re-balance data and re-map data to shards when the number of servers changes 


- goal of consistent hashing:
  - still spread data among shards
  - but provide flexibility for adding / changing servers

- imagine that both server ids and data can be hashed by the same hash function
  - servers can be distributed within the "hash space"
  - data can also be distributd within the "hash space"
  - the next server (for example) to the right will be the server that handles that response

  - re-balancing (adding/removing) servers doesn't affect ALL servers, just the "subsequent" servers

  - still have some problems with basic hashing:
    - servers or data may not be "evenly" distributed around the ring
      - there may be "hotspots" of data
      - servers may not be evenly distributed
  
  - hash functions - just have to produce output in the same hash space
    - could replicate hash functions to create "virtual server / database nodes"
    - typically hash functions would be pre-made and can be tailored to be applicable to your scenarios

- pros to consistent hashing:
  - solved problem of rebalancing a minimal amount of data when nodes enter/exit our cluster
  - can solve the challenge where nodes are not evenly distributed (using virtual nodes)
  - solves the problem of hotspots (using virtual nodes)

  
- trade offs to consistent hashing:
  - certain types of queries become almost impossible
  - a range-based query is now almost impossible - data has now been essentially distributed randomly
    - cross-shard queries are generally bad (expensive - resource intensive, lose ACID guarantees)

  - consistent hashing is typically used in NoSQL dbs since you wouldn't typically do range-based queries on non-relational dbs


- if the distribution isn't appropriate, then you can add additional hashes for virtual servers


- uses of consistent hashing:
  - akamai CDN
  - NoSQL sharding
  - could be used in message queues


- workarounds if you use a NoSQL db
  - e.g., trending tweets would be very hard to find in a noSQL db
  - could leverage cache, or could have specific "workers" that scan incoming data to aggregate and find in real-time
  - could federate data onto a different type of data store to make keyword search easier
  - ultimately, would have to use a different type of data store to facilitate specific use cases if the noSql db doesn't not work well


## Docker and networking
- cross-container communication is not a strength of docker
  - higher level dependencies are better
  - in production environments - docker containers are orchestrated by something else like kubernetes to manage IP addresses, etc.
  - cross-container communication isn't ideal; managing IP addresses is tough
  - cross-container IP addresses is hard - it might be something that isn't quite fixed

- there are no "right ways" to do things - there are only wrong things
  - come in with empathy - just contribute however you can
  - don't think there is a right way to do anything
  - not having something - it might be intentional
    - if things aren't that important - doing it right can be more effort

- requestBin project:
    - if you get it working, 
      - try splitting the db to another vps
      - or create another app server
      - there are a lot of ways to extend the basic app
    - try deploying to a different base_url
      - point nginx to different places
      - how you do things has ramifications for how to architect the app

- observability - need to monitor all of the things you have to look at (e.g., docker, servers, dns, db, etc.)
  - e.g., serverless observability - huge - not niche problems



