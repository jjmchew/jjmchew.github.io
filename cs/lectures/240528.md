# Tues May 28, 2024 (System Design Week 4 Day 2)

## Scaling 3-tier to N-tier
- how do we know when to scale the app server?
  - if the App is 250 mb, and RAM is 16gb
  - simplistically, could have up to 64 simultaneous instances which equates to 64 simultaneous users on a single app server
    - realistically, it will be less than 64: OS overhead, etc.
    - not all processes will be concurrent, etc.

- would we consider CPU usage? for scaling an app server?
  - we wouldn't typically worry about CPU usage
  - there is a balance of utilization vs performance
    - don't necessarily want 100% utilization
    - beyond 80% of *system resources* then it's good to spawn another server to ensure you don't max out, since things will slow down

- in production: spinning up servers are typically spun up automatically based upon thresholds
  - use of numbers is primarily for estimating current state

- thinking about concurrent users:
  - e.g., ticket master example:  14 million concurrent users doesn't necessarily mean 14 million / 64 simultaneous users (on each app server) = req'd number of app servers
  - in reality, the use of additional system components would reduce the number of simultaneous users than hit the app server
    - e.g., use of CDNs, use of cache, etc.
    - also remember that app servers are dynamically spun up depending on threshold values from traffic


## Performance
- see basecamp notes

- improving performance can delay scaling (which is a lot of work)
  - performance is a problem you have all the time (i.e., not a peak traffic thing)


- how do you handle performance of a slow page?
  - a common question in interviews
  - this is a performance issue
  - on the back-end, it's all about the db generally
  - performance isn't an objective standard, different things may need to happen depending on the page

- indexes change O(N) search to O(logN) by sorting data
  - we increase the amount of space used
  - increases write/update times

- N+1 issues:
  - when an app needs to get a set of related records, but executes those records in an inefficient way
    - e.g., JOIN would be more efficient than running N queries for the main records, and then another query for each of those records to get something else
    - latency in SQL queries is typically from the back and forth of opening / closing queries, hence N+1 is inefficient

- ORM:  Object Relational Mapping
  - creates a mapping between the SQL db and the object-oriented app code
  - ORMs can construct inefficient queries

- inefficient query calls can be especially problematic on the front end, if the react code is trying to make many slow queries over the internet


## Replication
- about solving the issue of too many requests for the data
- primary use read replicas
- could also use primary-primary replication
  - introduces conflict resolution requirements

## Partitioning
- solves the problem of too much data within a single store
  - hence the stores need to be split up

- in interviews, don't offer sharding or horizontal scaling of dbs unless you're being pushed that way
  - it's an irrevocable change - make sure you understand the gravity of that option
  - if you've done everything else you can do, then you can consider sharding
  - if sharding a "books" table, do we replicate the "authors" table?
    - would depend on use case - depends on query speeds, etc
    - could split up authors to correspond to the books shards
    - generally, want to minimize the cross-shard queries, otherwise, it's very difficult

- sharding is typically very app logic intensive
- too much sharding can be bad - increases the chances of cross-shard querying
  - so you don't want to shard too much, either





## Twitter in-class exercise
- sharding:
  - each shard will / can have it's own set of read replicas
  - want to avoid hotspots (read and write)
  - twitter - will probably be more reads

- with sharded databases:
  - don't forget that we need to add a data layer

- once you move to multiple app servers, you'll need to move the webserver to it's own node

- Nick sharded on user ID
  - included cache cluster for recently active users (pre-populated timelines), used LRU for eviction policy

- when sharding:
  - should think about space constraints, hotspots, and cross-shard queries
  - ideally, you can find a natural partition key that can provide all of the benefits
  - the exercise is really thinking about what the different options might be and what the pros/cons of each choice might be

- for interviews:
  - need to be aware of the potential weak-points in any decisions / choices



## Hash functions (Consistent hashing)
- just a function that takes any input and produce an output in a fixed range and the same string gives the same result

- hash functions are often used to "scramble" input

- most basic hash function is "modulo":
  - use modulo to determine which of 4 app servers to send a particular request to
  - could also use modulo to determine which database shard to assign data to (e.g., could has the unique_id from the db)

- why pick using a hash instead of using round robin:
  - round robin would require some concept of state
  - but if you use the hash function, don't need to worry about state

- if the number of servers / shards changes, then the hash function will re-direct to different instances
  - for app servers, this isn't so bad
  - for database shards, it doesn't work, you can't point to a different shard every time (you'd have to move all of your data)
  - don't want to have to re-balance data and re-map data to shards when the number of servers changes 


- goal of consistent hashing:
  - still spread data among shards
  - but provide flexibility for adding / changing servers

- imagine that both server ids and data can be hashed by the same hash function
  - servers can be distributed within the "hash space"
  - data can also be distributd within the "hash space"
  - the next server (for example) to the right will be the server that handles that response

  - re-balancing (adding/removing) servers doesn't affect ALL servers, just the "subsequent" servers

  - still have some problems with basic hashing:
    - servers or data may not be "evenly" distributed around the ring
      - there may be "hotspots" of data
      - servers may not be evenly distributed
  
  - hash functions - just have to produce output in the same hash space
    - could replicate hash functions to create "virtual server / database nodes"
    - typically hash functions would be pre-made and can be tailored to be applicable to your scenarios

- pros to consistent hashing:
  - solved problem of rebalancing a minimal amount of data when nodes enter/exit our cluster
  - can solve the challenge where nodes are not evenly distributed (using virtual nodes)
  - solves the problem of hotspots (using virtual nodes)

  
- trade offs to consistent hashing:
  - certain types of queries become almost impossible
  - a range-based query is now almost impossible - data has now been essentially distributed randomly
    - cross-shard queries are generally bad (expensive - resource intensive, lose ACID guarantees)

  - consistent hashing is typically used in NoSQL dbs since you wouldn't typically do range-based queries on non-relational dbs


- if the distribution isn't appropriate, then you can add additional hashes for virtual servers


- uses of consistent hashing:
  - akamai CDN
  - NoSQL sharding
  - could be used in message queues


- workarounds if you use a NoSQL db
  - e.g., trending tweets would be very hard to find in a noSQL db
  - could leverage cache, or could have specific "workers" that scan incoming data to aggregate and find in real-time
  - could federate data onto a different type of data store to make keyword search easier
  - ultimately, would have to use a different type of data store to facilitate specific use cases if the noSql db doesn't not work well



- consistent hashing isn't used as much for NoSQL dbs
  - we do think about sharding for NoSQL dbs, but we think about it a bit differently





## Q&A with Chris

### Docker and networking
- cross-container communication is not a strength of docker
  - higher level dependencies are better
  - in production environments - docker containers are orchestrated by something else like kubernetes to manage IP addresses, etc.
  - cross-container communication isn't ideal; managing IP addresses is tough
  - cross-container IP addresses is hard - it might be something that isn't quite fixed

- there are no "right ways" to do things - there are only wrong things
  - come in with empathy - just contribute however you can
  - don't think there is a right way to do anything
  - not having something - it might be intentional
    - if things aren't that important - doing it right can be more effort

- requestBin project:
    - if you get it working, 
      - try splitting the db to another vps
      - or create another app server
      - there are a lot of ways to extend the basic app
    - try deploying to a different base_url
      - point nginx to different places
      - how you do things has ramifications for how to architect the app

- observability - need to monitor all of the things you have to look at (e.g., docker, servers, dns, db, etc.)
  - e.g., serverless observability - huge - not niche problems





