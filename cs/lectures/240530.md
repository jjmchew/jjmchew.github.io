# Thu May 30, 2024 (Wk4D4)

## DBs
- materialized views:
  - a precomputed query that you will update as new data becomes available
  - this is analogous to the twitter timeline example:
    - the work to display the data is "pre-computed" at write-time so that it can be quickly displayed at "read-time"
  - would "pay the cost" of the table joins just once
  - Postgres has an implementation of this



## Distributed System Communication
- see basecamp notes

- microservices are roughly equivalent to Service Oriented Architecture (SOA)
- North-South orientation (traffic): roughly up and down the service / application (i.e., webserver on top, app server in the middle, db down below)
- East-West orientation (traffic): roughly communication between services

### API Gateway
- 1 point of ingress to centralize many things
- may include features specific to microservices (when used for microservices)
  - API gateways are also used for logging, authentication, etc.
  - helps to centralize functions so that individual services don't have to do this also
- drawback:
  - can be a bottleneck
  - can be a SPOF


### Service Mesh
- combo of "control tower" / "control plane" and "sidecar" proxy
  - fulfills many functions
  - 1 function includes circuit-breaking

- circuit-breaking: if a service goes down, there is a mechanism that will stop routing requests to it

- drawbacks:
  - a sidecar includes another network hop to communicate between the sidecar and the service itself
    - typically, sidecars are deployed very close (same system or region) to the service
  
  - control plane can be a single point of failure
  - implementation can be complicated


- benefits:
  - service mesh is like a "swiss-army knife" of different capabilities
    - can do canary deployments (gradually increasing % of traffic to a new deployment)


### Message Queue
- often pub/sub model
  - services subscribe to relevant events
- supports event sourcing
  - a log of events from the beginning of time can be replayed to get to a current state if a service goes down

- there are different types:
  - at-most once
  - at-least-once
  - exactly-once

- drawbacks:
  - generally are asynchronous - not ideal for events the MUST be executed
  - can act as a single point of failure and a bottleneck

- related capstone projects:
  - fjord
  - triage : head-of-line blocking
  - kuri : dead-letter queue-as-a-service



## Misc
- capstone write-ups
  - we may end up being more of a domain expert in our capstone project than even experienced devs
  - problem setup / background - needs to be very clear and understandable to setup the problem properly for everyone (including other devs)
  - not intended to be "simple" or for non-technical audience, just needs to be very clear



## In-class exercise
- create a microservices diagram
- think about:
  - common use cases
  - how does communication between microservices happen?
  - think about features to be a service
    - small, but not too much communication
    - sync vs async communication
      - e.g., playing a video might be sync, generating recommendations might be async (e.g., while someone is playing a video)
  - assume a "finished state" (not in transition)
  - have reasons for why you made decisions


### Spotify
- think about naming:
  - "recommendation service" : Nick tends to think about recommendations generated by the system for you
    - if this generates the homepage, might want to name this differently (e.g., "homepage service")
    - this service would reach out to other services (like playlists, recommendations, etc.) to populate the home page

- "front page service" might not need to coordinate everything on that page
  - i.e, a click on the front-page can go directly to the user playlist
      - it doesn't need to go back to users service before going to user playlists service
  - remember that services can connect directly in different ways (i.e., via service mesh, API gateway, etc.)
    - you don't need to communicate "through" existing services

- remember that services are different from infrastructure components, and different from db tables

### Uber
- it may help to think about events
  - think about what events occur and are used to trigger another action
- think about what needs to be sync vs async
  - can think about what can happen via messaging (e.g., rider signals an event that they are looking for a ride)
    - this can go to geolocation queue and then messaging for drivers can go out
    - these messages can be async
      - contrast that to spotify where pushing play might require a synchronous connection to play music

  - can think of a user submitting a request for an ride as an event which kickstarts a few different communications on the backend
    - these may take place in different ways

- uber payments:
  - settlement happens asynchronously - it's non-blocking
    - ride history and receipts come later
  - the payment verification:
    - if it stops your ride after it's started (since the card is expired), then it would be asynchronous
  - if payments are aggregated and settled at the end of the day, then it's definitely asynchronous

- communicating with users and drivers:
  - HTTP isn't the only paradigm to communicate
  - there may be websockets, server sent events, etc. that may keep services in better connection with other services
  - you don't have to use an HTTP request to kickstart requests
    - e.g., push notifications aren't HTTP requests

- services generally communicate together via an internal network (there should be no external public traffic)
  - typically okay to just expose IP address / port of the service and make that available for other services to use


## Jason Aricheta
- from New Zealand: did capstone from New Zealand
  - works for Hnry
- did capstone 2.5 yrs ago, now on normal hours
- platform engineer : a bit like devops
  - devops is about integrating tools so that application runs well and devs can push code into production
  - works in platform team
  - he looks after infrastructure
  - makes sure code platforms work well (e.g., push a feature to production) - should be as seamless as possible
- what's happening in the background (architecture) and then move to tools

- hnry : if you're a contractor, you would be an application to claim expenses, put in invoices with clients, etc.
  - makes it easier for self-employed / sole-proprietors

- app.hnry.io
  - you go to load balancer first (use round robin)
  - then goes to web tier (live app) - the application servers
    - will then check database to see if you're a legit user
  - there is a queue layer to hold jobs
    - e.g., there may be 3rd party api calls
    - the queue prevents the user experience from being very slow
    - workers will act on jobs in the queue

- they use AWS services
  - they use ALB (application load balancer)
    - http:  upgrade to https
    - they're a monolithic app, so this is easy
    - just direct to app servers
  - rails app
    - these are containers (lightweight virtual machines)
    - these are reliable and available
    - they use ECS (the aws version of kubernetes)
        - need to orchestrate containers
        - wish they had used kubernetes (would have been better for CV)
  - they use up to 200 different queues in the background
    - split queues into critical workers and non-critical workers
    - critical: cash management
    - non-critical: tax filings, routine checks, etc.

  - hnry interacts with a number of banks using background API calls
  - db is a single postgres db
    - they interact with a single db for consistency reasons

- when users use the app, actions are tracked
  - revenue, etc is tracked for investors
  - they use a datalake for aggregating data for execs, etc.
    - they use a lot of webhook calls (e.g., when booking an expense, etc.)
  - datadog and rollbar : for observability
    - use datadog for observability (after release, they check datadog to see if things go wrong)
    - rollbar : old, legacy
  - banks, government, email, etc. - use 3rd party API integrations

- platform team maintains infra using 'terraform' : Infrastructure as code
  - application can be tuned (mutate infrastructure)
  - through AWS UI
  - or through code : through code, there is better trackability / visibility for the team to see and so everyone knows what's going on

- they have a read replica on their DB
  - using fargate to manage
  - most users / devs only have read access, not write access (principle of least privilege)


- they use ECR image to build their CI/CD pipeline

- they use Route 53 and Cloudfront (CDN)
- also have a WAF firewall to keep out suss IPs


- in ECS / Kubernetes
  - if you have specific requirements for a container (e.g., 1 virtual CPU and 32 gb of memory)
  - the docker runtime will know the resource requirements of each container and how many containers you can pack into each machine
  - spinning up instances of virtual servces to meet container requirements
  - they also use EC2 instances
    - they don't use automatic scaling for EC2 instances (i.e., they don't scale compute)
    - it's not within the typical usage patterns - current patterns are very predictable

- live app
  - served by a container
  - when you submit forms, there are background jobs (with queues)
  - workers will pick up jobs
  - they have a DLQ (dead letter queue) for each queue
    - they use a lot of DLQs to see if there are sudden spikes, etc. which might kick-off an incident

- scheduled jobs
  - some jobs are enqueued periodically
  - these are checks to ensure that things are being sent regularly
    - e.g., doing consistency checks between banks and their own data
  - use EventBridge to kick off periodic jobs (e.g., daily, every 10 mins, during the month)


- queues are cheap
  - and in SQS, it's easy to separate jobs
  - background workers are very efficiently monitoring the queues using rails
    - workers execute the jobs based on the specific queue that they pick up jobs from


- rails is NOT a single page application
  - the dashboard is rendered at the time you read it
  - it's similar to handlebars
  - they use indexing:
    - you can go from several second queries to microseconds using cache


- why split critical and non-critical instances?
  - they use spot instances for compute on non-critical instances
  - critical jobs are on dedicated instances - they need higher reliability for those (can't afford for that compute to get cancelled like in spot instances on AWS)


- CICD and pushing changes to production
  - code gets pushed to github
  - some events occur
  - circle CI (3rd party provider)
    - dependency installation (install all js / ruby dependencies)
    - a lot of testing!
      - even if you change the colour of the button, you have to test everything
        - this hampers things
        - if you decouple the app you might not need to test everything
    - build an image
  - push the image to ECR (aws version of docker hub)
  - deploy to ECS (pull images from registry)
    - they slowly creates new instances (of new version)
    - then slowly terminates old instances (of old versions)
    - they refresh containers about 30 times / day
      - as soon a dev pushes new code, the goal is to get it to production as soon as possible
  - using ALB
    - they can direct traffic to specific containers (old versions vs new versions)
    - queues also distinguish between old and new containers
    - generally try to make changes as incremental as possible so that old/new containers aren't an issue

- they don't use distributed caching layer
  - they use sticky sessions using ALB, so don't need distributed caching
  - no autoscaling
  - primary db serves both read and writes
    - next week, they'll vertically scale the db (@ 4am)
      - they have more users and they'll scale gradually, but CPU limits are at 80 - 90%, so there are some inconsistencies in the db and transactions are starting to terminate
      - they have a read replica, but it's primarily for devs, not for production jobs
        - their db replication lag is up to 500ms - it's bad, could be possible to improve that in EC2, but they haven't done that yet
    - they'll be launching in the UK soon - so they'll be a multi-region

    - they run pretty lean (i.e., not many engineers) - so they've chosen to NOT have some components
      - they're choosing not to have these components
      - if they get more users, they'll have to start making changes
      - they're keeping things as simple as they can, for as long as they can

    - for sticky sessions: they have a small memory store on each virtual machine itself for the app server
      - once the metrics from the app server show that traffic exceeds the available memory on each server instance, they'll probably introduce caching

- hnry:  20k users in both New Zealand / Aus
  - active users peak: 10k daily
  - requests:  300k avg / hour
  - latency: avg is 0.3 ms, peak, almost 137 ms peak
  - p95: more than half of requests are satisfied in 11 ms
    - max request time was 13 mins
    - most requests were done by the load balancer
      - load balancer knows that containers are terminating via health checks
        - 27M requests / month from health checks
  - sticky sessions are using web sockets

  - if there's too much traffic, they will get 500s - they don't have that usage pattern, don't expect to see it
    - web containers are quite efficient
    - it would be a good problem to have

- they watch for 500s to know when stuff has crashed and an incident is starting
  - they have alarms on the 5xx response to HTTP requests

- they see a lot of people trying to access git.config at IP addresses


- capstone project helped a lot
  - dealt with containers a lot
  - preparing for the role and job interviews, you get to consume ever more material
  - that will give you a better sense of all of these systems
  - blue/green deployments, etc.
  - from understanding the high-level architecture you can better figure out the lower stuff later
  - once you're on the job, you can try and talk to other engineers about the assumptions
    - e.g., are containers actually talking?  how are async jobs performed?
    - being able to talk about it architecturally was very important for the interviews
  - starting the job, you really just need the high-level



## Homework

Readings, Videos, Homework:
We're going to round out our study of event-drive architecture with the below talk from our 2nd (or 1st) favorite Martin.

    Watch - [Martin Fowler talk about EDA patterns](https://www.youtube.com/watch?v=STKCRSUsyP0&vl=en)
    (if you want a high level overview of EDA) Read - [Best Practices for Event-Driven Microservice Architecture](https://dzone.com/articles/best-practices-for-event-driven-microservice-archi)


Group Work

    Finish Request Bin! We'll be having teams demo their RequestBin tomorrow.
