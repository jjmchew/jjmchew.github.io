# Fri Jun 28, 2024 - W8D5

## Discussion on numDialer
- if you haven't finished the project and you want to work on it next week, just DM Srdjan if we have questions

- key decision discussed yesterday: client-server must exchange info
  - don't implement SSE on a fixed interval (don't send message that aren't necesary)
  - only send an event when something happens (i.e., when the status is updated)

- other decisions:
  - shouldn't need the client application (i.e., front-end)
    - can just send HTML from the backend to the front end with a button and then push updates from the server
  - initial 3 calls need to be concurrent
    - i.e., a student put a `for` loop with an `await` keyword
      - promises were resolving quickly, but the calls weren't concurrent
      - could start each of the first 3 calls using `Promise.all`
  - subsequent calls need to be initiated once that call is *completed* not once the first promise is resolved (since the first status is 'ringing')
  - some statuses may be out-of-order since there may be latency issues with status messages from API arriving differently
    - just look for "completed" - it was a design decision on handling this
    - so no need to make updates on every status

- viable solutions:
  - could use react on front-end, could use websockets : this student went to the next round even if the solution might be a bit less ideal



## Meeting w/ Chris
- wants us to get the lay of the land
- thinks the coding part will be the easy part
- read the articles - wants to hear thoughts
- will make a learning plan
  - we need to be power users with the 4o model
  - need to be a user
  - need to learn how to use coding assistance
    - at the conference had coding assistance, like copilot, cursor (amazing, will use it for launchschool.com), cody (by sourcegraph, big presence at conference)
    - checkout github copilot and cursor

- keynote by OpenAI:
  - use the desktop app (4o) model
  - opened up a book - turned on the camera
  - showed the book to openAI for 1 sec, then took it away
  - "summarize the page" - that's how fast the OCR was
  - opened the code-editor: screenshare
    - talking to the editor - I'm stuck on this, what do you think
    - opened more tabs so that the tabs show up on the screen - the AI was building context
      - the AI was pair programming with you - it could see theh code and build more understanding of the code by looking at it
    - as you flip through the pages, the AI can help you
    - someone asked "do you use coding assistance"?  the speaker said I use "cursor" - I like to see how OpenAI and cursor interact


- we need to be a user of the tools
  - as a developer, we should use these tools
  - become a power-user

- should do a prompt-engineering course
  - look up vocabulary (e.g., CoT, zero-shot, etc.)
  - just need to know generally know it is
  - prompt engineering is being a power user
  - this is part of the RAG world (more complicated than just chatting)

- read a RAG book - don't have a resource in mind, yet
  - will look up a course
  - hot area is "agentic stuff" - we won't go there
  - knowledge graphs - lets not worry about that yet

  - RAG pipelines - could be the richest area for us
    - deploying it into production with various components
    - compartmentalizing each part so you can change it and experiment with it
    - change out a piece and experiment with it
    - same as a "data pipeline" - pipeline means multiple components (vs "1 thing")
    - adding adapters makes it a pipeline
      - talks about deploying into production
    - as a guess - thinks this is a good direction to go
      - this enables more than chatting with the document (the "hello world" of RAG)
      - need to enable more features
        - could have model drop-down - add new models
        - chat with a selection of sub-docs - manage context, data collections, etc.
          - different docs in different pipelines
        - could think about pipeline / sources
- we need to know where RAG sits as a methodology
- one workshop (3 or 4 hours) - will walk us through fine-tuning
  - if RAG doesn't work, need to know about fine-tuning
  - after that, there are more tools, methods, productionize the fine-tuned model
  - how do I call an API against a model
    - depends on whether or not it's for 1 person or like chatgpt

- going through a full course might take too long, there might be some videos, etc.
- that's our plan for the next week or 2 weeks
  - we need the lay of the land, vocab, the larger ecosystem, need to refine it through podcasts, etc.

- we can be a full participant since this field is so new
  - there are practices, but don't know how long it will be a "best" practice
  - everyone is just learning in this space
  - that's what makes it exciting

- the guy who through this conference was doing front-end stuff 2 yrs ago, was a finance guy, took a bootcamp and become an AI guy and put on this conference
  - he's a fan of bootcamps and launchschool, went through a transition
  - when he started doing AI he became an influencer
  - there's a lot of opportunity
  - people are excited that you're excited
  - at the conference, everyone is excited - if you're not excited, people don't want to chat


- Grace:  RAG-as-a-pipeline:  scaffolding?  Tools?
  - CL:  it could be.  let's keep it open.  it's hard to change, experimentation, iteration, new dbs, new models, do you rebuild this every time?
  - for internal experimentation, we could use this, it would make it more effective
  - a better model will do a better job, regardless of rag architecture
  - could enable better experimentation
  - could build plug-in architecture
  - a lot of this will rely on core system design knowledge
    - e.g., multi-agent architecture:  agents as microservices - this is normal system design stuff
    - next step is "service mesh".... could be an "agent mesh" next
    - any new thing has problems, solutions will come up
  - e.g., people ask LS if they've tried x..... don't do that.  LS has been around and tried many things
    - we need to become a bit native to this place before offering a solution

- that term AI engineer: as some negative connotations
  - lays things out generally
  - see article

- Grace: this should open us to engineering roles and AI engineers
  - CL:  for non AI roles, people might have to clarify that they aren't AI-related "are you okay with that"
  - thinks that companies will be AI-curious, full-invested, or AI-native
  - there probably aren't any companies that aren't AI-curious
  - when serverless first came out, people were concerned about serverless projects
  - CL:  at minimum, companies are at least curious or beyond curious.  it should help more than it hurts

- JC:  testing
  - CL:  in AI world:  evals - massive topic
  - can have unit testing
  - models are probabalistic
  - evals only work with standard input and output
  - chat with doc is hard to be progammatic about
    - hence there are limits - response should have a word to a certain query
  - HITL - human in the loop:  humans perform evals, expensive, can't do it a lot, need to sample (hence stats)
  - LLM as judge - controversial:  more scalable
    - there have been studies, models have biases for themselves, need to use a different model
  - testing is hard
  - testing the result of LLM output - evals
  - humans have biases, etc. also
  - we need to be aware of it, need to have an API, etc.
    - should be able to plug in


- JC: code as shell vs LLM as shell
  - CL:  things don't stay in their lane
  - DBs are primitives in development, but what if postgres starting making web apps
  - CL:  primitives don't exist, yet
    - don't get too caught up in words
    - people are just trying to find patterns
    - there are probably more patterns than this
    - shell analogy might not work in future
    - there are a lot of mental models - need to look at the date of publication - it might just be in the moment
    - RAG as architecture is an established "primitive"
      - as much as there is
    - context-window may affect this, but no one knows
  
- hosting models
  - CL:  hosting models is tough
    - requires GPU processing
    - there is a lot more you have to do to serve lots of requests
    - if you can contain the use case - e.g., for small internal teams, can't be a big service
    - it's not impossible - there are tools to self-host and serve models
    - to automate that it would be fine, but it relies on GPU
    - has a PDF on that - could be a possibility
    - automating the setup onto EC2 instance
    - could help you set it up on managed providers that host models (we'll do it for you)
      - e.g., Octa AI - that's what they do
      - google vertex AI
      - they take a model and call it via API
      - our thing is packaged with octo and vertex and we can add more later
    - working on laptop is one thing, serving a team is another thing
    - can automate - this is what octo does

    - doing this involves telemetry, observability, when you put your models there and something happens, they do tracing, logging, etc.
      - depends on where our point of emphasis is
      - the whole pipeline vs RAG - might want to use octo, etc.
      - the more work / context you use - the more computation is required

  - LS:  want to do RAG to refine questions in the language of launchschool
    - wants it to answer a particular way
    - also want to say if you want to learn more, read these assignments
    - e.g., if someone asks a question about a course.  the bot should say don't worry about it
    - the documents are fend into the prompt based upon the course that people are in

- CL:  probably don't want to do the model hosting story
  - after preliminary research - it's VERY hard
  - using someone that can host models for you
  - CL will definitely use a provider and NOT EC2
  - if you can build something with observability, etc. then I might use it
    - if you can build it OSS then maybe
    - but Octo is cheap
    - it's hard to justify doing my own

  - this is good timing - LS is doing their own research
  - project needs to fit the existing mental model pepole have - don't want people to have to "suspend disbelief"

- Grace:  next steps - do more readings, look for problems?
  - CL:  don't look for anything:  just use the tools, look around, don't commit to any neighbourhood
  - use the tools
  - if we have time, do the Deep Learning (Andrew Ng?) course on prompt engineering
  - any decent sequence will do
    - zero-shot, few-shot, CoT
    - needs to cover these things

  - CL:  the goal here - don't pretend that you once lived in new york (don't have watched a lot of videos of people who lived in new york)
    - we need a "2 month road trip to new york" - not pretending or watched movies about new york
    - need to be real
    - need to be at the state of the art
    - the downside is taht this will be the first capstone project in this space
    - CL will err on the side of you need more than less
      - tell me if it's too much
      - this might run over the allotted time
      - the project can possibly expand and collapse
    - CL - friends with Simon Williamson - we need to make some progress, and he might be able to provide some advice
      - might be able to pull people in and bounce ideas off
      - the risk isn't too little scope
      - risk is too much scope
      - pushing it the wrong way for conversation
      - it will definitely be complicated enough
      - want to be able to answer "have you considered this?" - should have an answer
        - everything is exploratory and curiosity-driven - once you have a good knowledge base, it should be friendly, and good
        - no ones really an expert
        - if you are an expert you realize how quickly things are moving

- CL:  important things will come up again
  - the ROI on re-watching recorded meetings is low

- CL - abstractions bleed out to application developers
  - we need to be aware of dataset, parameters, etc
  - tokenization, chunking, etc. - not too low-level
  - see youtube video on tokenization, etc.
    - talks to software devs - provides an overview
    - this is too deep - we could know it, abstractions leak out
      - wouldn't go too much farther than that
      - could talk about how to train your own LLM
      - after that, do something RAG-specific - pipelines, etc.
  - the genAI course is right on target
  - read the articles first - then do MS course

