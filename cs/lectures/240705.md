# Jul 5, 2024 - meeting w/ Chris

- good discussion on slack around learning and discovering model projects
  - you can only get so far with reading
  - depending how far we got on reading about RAG, some stuff might just look foreign
    - the knowledge gap here might be a bit larger
  - if we're crunched for time - read the RAG article and watch the RAG video
  
- biggest concern is not that we won't build something
  - biggest fear is that when we go explain it, and we don't fully understand why a decision was made or what it was
  - if we pick a project, we can say this is how LangChain does it, and as a result, we liked it or didn't like it
  - the interviewer likely won't want to "argue with LangChain"
  - the star of the show isn't the project - it's us
  - we want to elevate ourselves
  - we don't need users for our project
  - when we're trying to get users and make money - hence something novel is great - we'll be the only people there
    - hence, it's worth the attempt; investors will put money into it to be first
    - for us it's a bit different - we don't need to try anything new
    - we're not betting on something new
    - we're only interested in pitching something that already exists in a more niche use case
    - in traditional capstone domains, it's safer

- for us in AI, it's still a risk
  - things are changing fast so projects that are established that look popular can be wrong fairly quickly
  - there's a little bit of danger in this

- how do we bridge the gap between not reinventing the wheel?
  - CL:  we should re-invent the wheel
  - need to find an established product
  - re-invent the wheel enough to understand the problem domain
  - need to understand the domain
  - if we pick a random project and re-build it, but we don't understand the larger problem context then we won't be able to talk about the problems

- CL first start-up in his 20s
  - an interviewing application
  - 2 people built the whole thing end-to-end
  - very interested in the tech
  - didn't have any interest in the domain
  - got the idea from a hackathon and built this thing
  - people said turn this into a company
  - concept : send out questionnaires and people answered it by audio and video
    - people could also answer by phone call
    - hopefully most people were video, some might be audio only
    - a video-interviewing application
    - had fun building it
    - had airbnb as customers, 1 laptop / child as customer, etc.
    - made money: $20k - $30k / month
    - a current company:  hireview (sp?)
      - a very similar concept
    - at the time - reached feature parity in 2 months
    - CEO called them - wanted to meet
    - did a lot of sales calls with senior execs (e.g., TJMaxx, etc.)
      - they were evaluating video interviewing options
      - gave a great demo - coded a feature after the fact
      - thought the sales call was great, but they went with someone else - b/c the customer didn't understand them
        - the unlimited plan ended up being too cheap, didn't understand that payment could be anything
        - didn't understand that CEO was the account rep - they wanted someone to call
        - it was not understanding enterprise sales
    - remember doing sales calls with recruiters who used his product
      - there were fancy exec search firms - charge a lot per placement
      - $499 plan included a white-label - no branding
      - they were selling the interviews for $500 - $1000 at a shot, but they weren't recruiters
      - do you plug into ATS?
        - how do you not know what an ATS is?
        - the recruiter had to explain - you are "pre-screening" - you're a small part of the overall process
          - e.g., pre-screening, interviewing, on-boarding, employee management, off-boarding, etc.
          - they want all of the data centralized
          - not understanding HR was the issue - CL didn't want to know about HR and ATS and recruiting
          - to make it grow, they would have needed to make plugins to multiple ATS, mobile apps, etc.
          - they didn't care about the industry
          - had to pick the industry first - needed to get into the industry

    - for us, it's not just the product and tech
      - we need to be a native of that domain
      - can't give answers that betray our lack of familiarity
      - can't just mimic without background work - don't want to be like CL in recruiting


- being native in this space:
  - everyone should explore in different areas - don't just explore in eval
  - have a few people looking at different things to hedge our bets
  - CL:  prefers having multiple avenues to look at
  - CL:  thinks evals are interesting and a huge topic of concern
    - you're right that the boundaries are not established - there are lot of things to experiment on
    - since everyone is experimenting there are a tonne of things to play with, so everyone is experimenting
    - there's a lot for application developers to contribute
    - we'll have to figure out where that is
    - capstone grads have the infrastructure side
      - so don't do pure libraries - that hasn't traditionally been good for capstone grads
  
  - a better thing to do would be to let me give you a tutorial on this, provide a demo
    - try to catch each other up on the learning
    - need to do a demo
    - don't share as many links
    - "let me catch you up and share what I've learned"
    - create a little video, etc.  keep it in an "eval" bucket

  - work on getting people in the same place
    - lead with demos:  show people what it looks like to use this tool
      - make it tangible
  
  - piercing through marketing copy is very hard
    - hard to understand, everyone claims everything
  
- CL:  people are marketing since there's a lot of money at stake - they need to make a bigger claim
  - they got $300m in funding
  - there's a lot of money at stake, so you especially need to have x-ray glasses on
  - people might do the same things to chase the money, even if the core of the product hasn't changed

- CL:  wants to slowly ramp down the learning
  - but there needs to be a continued undercurrent of learning
  - our individual conversations will be the test - hence keep learning
  - even after the project, need to do the fast.ai course and keep learning

- GL:  was looking at flowise.ai, no-code, etc.
  - being able to interchange models, etc.
  - CL:  a front-end exists
  - GL:  has flowise running on her machine
  - CL:  demos are the key to this part, reading and copy can only get you so far
    - CL:  is there an infra story?
    - GL:  not sure
    - CL:  when he sees something like this - he asks - why does it exist?  Why are people using this?
      - need to know top 10 ways - need to be exposed to those problems
      - this is a hint to the problems that exist
      - who is using it to solve what problems?
      - then, we have an appreciation for more problems
      - the top 10 reasons:  we can then zoom into this and look at how people solved those problems before this tool
    - this could be just a LangChain or LlamaIndex "wrapper"
      - if so, then it could be a great capstone project - just create a wrapper on existing orchestration tools

- CL:  need to craft a very careful story around the "open-source langsmith"
  - this could be under the "deployment" bucket
  - part of the ideation phase
  - we're repeating that process here since it's such a large space


- next week:  need to have some demos, etc.
  - need to understand things at the core better and understand the projects
  - need to have at least 1 demo per day
  - can alternate people
  - will aim to have at least 1 demo per day




