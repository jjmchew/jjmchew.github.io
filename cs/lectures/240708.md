# Mon Jul 8, 2024 - W10D1

## meeting w/ Chris

- Ben demo
  - DeepEval: a testing framework
  - used for evaluating LLM inputs/outputs, and retrieved documents in RAG
  - CL:  we're not building a library for capstone
    - we can't build a library
    - no infrastructure story there
    - we can't build code that other people just suck into their project
  - a model project:  needs to involve some sort of logging in
    - need to login somewhere and use the tool
    - if you don't login, then need to be aware that it's not a model project
  - CL:  let's login to see the results of tests
  - GL:  need to wrap API?

  - CL:  hosted application in the cloud gives us an opportunity
    - don't demo if you don't know the tool
    - if it's not ready, just demo tomorrow
  
  - interested in:  
    - we're a software eng team - we're going to build a competitor to this
    - imagine CL is the manager
    - team can't build something if we don't understand it
    - need to pierce through a bit more

- the kind of analysis CL is looking for:

  - hosted environment is interesting
  - DeepEval is a company - that's a good sign
  - if we're going to build this, we need to have answers
  - hypothesis - looks like a company that is hosting eval results
    - wants you to use their hosted results
    - to make it easy, they give you a library to include in the project
    - try to make it easy to generate evals
    - if you use their library it automagically uploads things to the cloud
    - eventually they'll want to charge you for different features
    - that could be a project since there's a hosting story and an SDK story
    - with the hosting story you have the opportunity to create SDKs in different langauges


- CL:  not interested in the code and how it works (for now)
  - tier 1:  *what* is it?  who uses it?  (is it popular?) 
  - having users provides validation of the problem space
  - if there are users, then worth looking at it to figure out
  - if there are users, there are problems


- GL demo:
  - flowiseAI
  - it's a UI wrapper for LangChain
  - LangChain is a coding framework with pieces relevant to LLMs
  - setup a RAG system:
    - text splitter
    - connected to PDF input
    - connected to a vector DB (hosted pinecone instance)
    - could use a local model to do embeddings, etc.
    - could also use OpenAIEmbeddings
    - Retrieval QA chain - will retrieve from a vectorDB and generate a response
    - can include Redis, etc.
  - flowise is an NPM package - you spin it up on your local device and you get the building UI
  - after you save changes and insert docs, etc.
  - the chat will be ready
  - it provides a dev chat portal
  - flowise doesn't deploy for you
  - so you can run the setup and it creates an API endpoint for you
  - you could serve through exposed flowise endpoint

  - positioned as a no-code solution, but you'll have to have some basic code knowledge for deployment

  - CL:  can you see the generated LangChain code?
    - GL: not sure
    - CL:  Flowise is a Y-combinator company - this is a good thing - it means they've raised money on this
      - it won't be "too simple"
      - YC companies are very ambitious, it won't be just open-source
      - to get funded someone has validated the problem
      - probably once you build the app, it could be like heroku - there's a hosting story
      - once you're in, you're in (once you're hosted)
    - if you can setup RAG quickly, then with 1 click, you should be able to deploy quickly (e.g., metered pricing, hosting, etc.)

    - this is a common "open-core" business model
      - e.g., gatsby, bought by netlify
      - mongoDB - use for free, but there's a hosting story

    - make the default hosting option their own

  - CL:  is flowise popular?
    - GL:  20k stars on github
  - CL:  how many people are building low-code LLM apps in this way?
    - this has a strong AI story
    - it's primarily a software eng story
    - hard part is that drag and drop thing, orchestrating LangChain code might not be straightforward
    - LS started using LangChain - it's as robust library - closed $30m 6 months ago
      - open-source
      - they changed a lot of things, and LS got caught in the middle - they were doing too much
      - too many layers of abstraction and those layers kept changing
      - so LS ripped out LangChain, and started rebuilding those abstractions to use a model router, etc.
      - so now, LS is rebuilding some of those layers of abstractions and they understand it now
    - LangChain tries to anticipate all use cases for everyone
    - this is an interesting story
    - this is very complex since the UI is a series of selections
    - without the fancy UI it would be a series of choices (e.g., like a Terraform config file)
      - the drag and drop is modifying those configs under the hood
      - in some ways the flexibility is hard - there are potentially too many ways to connect, etc.
      - the product is fairly simple - so making things simpler might be a weird pitch

    - if you can get LangChain code
      - it could be like LlamaIndex (another competitor)
    
    - this is great for experimenting (that's very slow)
      - to have a visual to experiment is amazing
      - don't want to be stuck with flowiseAI
      - if it generates LangChain code - then you're not stuck - the UI would then just generate code, which you can always work with
      - CL:  wouldn't want to be stuck with flowise since you don't know how long they'll be around
      - UI is proprietary

      - CL:  interested in seeing what code is created
        - GL:  looks to be a json file produced
        - GL:  they don't have deployment yet - docs are just deploying onto other providers (e.g., like DigitalOcean, etc.)

    - CL:  it's an apache 2 licence
      - means no one else can host?
      - not sure?

    - CL:  look for competitors to flowise
      - compare and contrast this
      - see how they compare themselves to flowise or vice versa
      - might give hints to how competing products view themselves in this ecosystem

    - CL:  thinks it has legs, but doesn't quite understand yet what code is generated after connecting all the pieces with drag and drop

    - it could also be their own "glue code" (which is their product)

    - wants to know if it's possible to dump the UI and go straight to the LangChain code

    - likes the autoconfig of vector dbs, doesn't love UI generating code - that's hard - a lot of coding

    - very front-end heavy
    - CL:  if you're not excited about the front-end, then don't choose this one
      - or pick another method of config
      - low-code doesn't have to be react UI, it could also be a command line - "answer these 10 questions"
      - just need to make it easy to generate code for RAG
      - can this UI also generate LlamaIndex?  Can it just somehow generate new code?
      - CL:  wants to know just how "awesome" this is



- library vs command-line UI:  no deployment story, no company behind it
  - it's only code, just `npm install`
  - if it's a YC company, there's got to be a way to make money
  - logging in is typically important
  - the library can't be the main project



- JC:  talked about Haystack
  - not exciting, but could be useful
  - CL:  hard to disambiguate between vendors
    - they all said the same thing
    - people at the booth were all sales people

- CL:
  - our story should be a little bit easier:  if you want to self-host, you shouldn't have to log into AWS and setup every service
  - likes what Grace showed:  showed that it's hard to experiment
    - there aren't a lot of tools out there
    - in web dev, it's easy to experiment (e.g., just use Postgres)
    - in AI apps, not sure what's best, not sure what embedding model to use
      - trying a bunch of different things is hard
      - having a model router - LS wrote their own
      - every abstraction they write costs a day
      - with an app to mess around, they won't have to write their own thing
    - to deploy
      - want to pick and choose
      - definitely want to pay someone to do this
      - RAG pipeline - want experts to manage this full-time, but want to pick and choose
    - like the visual of the boxes to see the components of the pipeline and get the choices
      - as a dev - don't want to be stuck with the tool
      - want to peak at code and ditch the tool if necessary
      - don't want to be a flowise expert
      - you could be a LangChain expert (it's big enough)

  - wants to see how that UI results in the code - if it's easy, then we can do it, too
    - if it's crazy hard, we'll find something easier
    - or we can do something totally different
  
    - it might be worth requesting access and checking the hosted version of flowise
    - they don't make it easy - they make it thorough
      - it's likely to be much easier to deploy when you log in

    - our story could be just deploying to AWS much easier

- don't want to get stuck on Flowise too early

- VerbaAI: open-source project
  - developed from Weaviate
  - "goldenverba"
  - designed for companies to keep things local and not on the cloud
  - looks like they want you to use Weaviate

  - CL:  they call this "end-2-end RAG in a box"
    - how many of these things exist out there besides this one?
    - where does this RAG in a box sit in the larger context of RAG in a box projects?
      - main question
    - CL:  it looks solid - that's a good sign
      - who's using it?
      - how many of these products are there?
      - it's likely to be pushing weaviate (that's their angle)
    - for capstone project, we could do "rag in a box"
      - we need to know the landscape
      - why did you build this?
      - need to be familiar with other frameworks

    - need to know the top 5 and the tradeoffs - need to be an "expert" in this

- CL:  as we study these projects
  - use the hint that the project exists and use that to validate the problem space
  - then explore the problem-space
  - don't just look at 1 thing - it's hard to tell if 1 thing is good or bad
  - hard to tell what the tradeoffs are
  - we can only understand this when we contrast it to other things
    - by just looking at 1 thing it's hard to understand it
  - the problem-space may be a subset of a larger problem space
  - need to understand our place in the problem space


- for flowise:
  - it's not drag and drop, the problem space is low-code RAG development

- any of these ideas
  - flowise, verba, etc.
  - any of these could work as a project, as long as we understand the space
  - CL:  I built a visual analyzer of different algorithms (a front-end thing)
    - this was common when bootcamps first came out
    - initially, people thought that was incredible
    - if we can build half of flowise, it might be a bit like building a visual algo analyzer
    - these aren't dead-ends, it's just a matter of team preference and clarity in terms of where we go


- these iterations cost a whole day, so make sure you bring something tomorrow
  - better to go long and review a lot of things

- bentoML
  - CL:  okay to zoom out from infrastructure standpoint
    - but don't want to move into fine-tuning, etc. - don't want to get into that world
    - infrastructure is just more sophisticated RAG - still doing retrieval
    - sophisticated RAG should have evals
      - you can have an app without tests, but a sophisticated app should have tests


- bentoML
  - focused on 1 feature:  deploying models into the cloud
    - models used for training are much larger than ones on the web
    - serializing models, etc.
  - parallelizing the model - so you can horizontally scale instances
    - run them in parallel instead of a giant GPU
    - CL: if we're going to be in this world - lower level concerns bleed through
      - e.g., CPU vs GPU - we need to be well-versed in this world, which can be quite difficult
      - just hosting models, you need to have a story on being different, be a deep expert - which is why you would use bentoML
      - we probably won't be able to do this as capstone project
        - we'd have to find an infrastructure provider who can do this
        - generally, capstone projects are a convenience story
        - LORA : chopped down models, etc.
        - e.g., create a place to deploy LORAs - this is probably a space where there are a lot of PhDs
      - OctoAI:  they fine-tune then host models
        - e.g., take a base model from HuggingFace, fine tune with your data, then they produce a new model with an API
        - co-founder was a PhD
        - also spoke with Google Vertex - they host, but don't fine-tune - for ML people to manage all their models
          - it's a whole world which requires a different training curriculum
      - not sure the Capstone profile is ready for fine-tuning - it's much lower-level
      - it's likely easier to become experts in RAG than fine-tuning

  - backgrounds for flowise people - likely not PhD researchers
    - if flowise is not PhD and they're devs is likely a better fit



