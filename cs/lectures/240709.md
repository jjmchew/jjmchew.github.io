# Tues Jul 9, 2024

## meeting w/ Chris

- are we still reading and learning?
  - are we maximizing our days?
  - are we doing things full-time?  That's a lot of time
  - don't waste a day
  - we are behind the other teams by about a week - want to hone down to a project

- Ben demo on evaluation space
  - no silver bullet metrics
  - products are blackboxing the eval
  - or giving standard eval metrics
  - store datasets for testing and store results of testing
  - allow you to store inputs / outputs for app
  - noticed trend of being open-source for evals
  - enterprise in terms of production observation
  - deepchecks - couldn't get a trial
    - enterprise observations and webhooks
  - deepEval allows tracing to debug outputs

  - deepEval - 2.4k stars on github
    - all competitors might be 12k stars between 4 eval companies
    - have some metrics out-of-the box
    - allow some custom metrics (need paid version)
  - CL:  what's the value?  tracking drift over time?
    - BB:  yes
    - CL:  how do they track drift?  Who is the judge of that?
    - BB:  for deepeval - it doesn't have much more than giving you an observation platform
    - CL:  humans are the judge
- CL:  generally for evals
  - programmatic (script, etc.) : something you can code (e.g., response contains a word or phrase), like unit tests
  - HITL (Human in the Loop) : people decide
    - it's the best, but it's the most expensive
  - LLM-as-judge : LLMs decide if the results are suitable

- CL:  LS currently doesn't have an eval framework
  - just winging it - how does it "feel"?
  - not sure about what's better or worse
  - issue:  they're not moving fast, so they may not need this
  - the may a change every week - and then they wing it
  - if dev velocity, something more systematic would be required
  - if tweaks were being made everyday, then they would definitely need a system
  - so these tools are very important
  - e.g., made a change on Jul 5, want to see what the effect is as a result

- refactoring infra isn't something that you do
  - you would need to plan that change to a db

- eval frameworks - could push into LLM-as-judge
  - people might need more on-boarding, there are few power users at the moment
  - it could be like coderpad
    - coderpad has pre-built questions out-of-the-box
    - it was built for corporate interviewers, Software Engineers who need to interview someone - here are a few questions
    - creating out-of-the-box experience is another option
  - could use our LLM as judge and then walk people through it

  - eval hyperparameters : values that are given to the test which prompt template, etc. to keep track of observed table of tests
    - it's not automated, just providing metadata

- CL:  can think about this as if you were a consultant
  - need to do a competitive analysis, etc
  - need to find examples that fit the use case better
  - researching with no anchor is tough - pretend that you're a user
  - LS is a good size - small enough to understand the scale, etc.
  - try to anchor things to what we know

  - evals:  should be a rich opportunity
  - what should LS pick and why?
  - cost-wise, feature wise, etc.

  - cost is a key priority for LS
    - could be a large platform or a set of libraries
    - it could be a bit decision - might want to spend a week on it

  - wants to see demos - wants to know what to chop
    - if we see e2e products - it can be confusing
    - either need to pick a subset - find smaller projects
  
    - deepeval exists and its being used demonstrates that it is enough
    - don't need to feel like we need to build an end-to-end thing
      - it would be impossible
    - don't want to be met with disbelief
      - don't build for breadth
      - be very targeted
      - needs to make sense
      - needs to have an infra story

- CL: doing research
  - looking for a new editor for the app
  - i.e., LS editor is a markdown parser
  - they didn't write their own parser - use custom markdown tags for their staff
  - there are different editors to make it better and support code, etc.
  - tiptap was recommended
    - they have an AI story
    - editor has comments
    - seems like overkill
    - then asked, how much does it cost?  $1000 / month for an editor
    - business model is open-core
  
  - this could be a capstone project
    - if you add a hosted component - "i.e., bring your own cloud - bring your keys" then you have a lightweight tiptap
    - bring your own models - to have your own features
    - that could be a project

  - there are projects everywhere
    - you need to be excited about the project
    - it needs to be containable
    - if you look at infra and it's too confusing, then switch to products
    - don't get overwhelmed
    - don't have too much anxiety - it might be easier to assign something

  - if we don't pick evals, then we might not get any more exposure to it
    - use the presentations to learn about the ecosystem
    - this might be it for some of these topics

  - make sure we get at least 1 demo each day (8 hrs) since we might not touch them again


- GL:  challenge perceptions of front-end
  - CL: flowise is a lot of front-end - feels like a lot
    - feels like that's not what we want to push on
    - deepEval : doesn't rely on front-end - should be much easier (it's a dashboard)
    - flowise - it's drag and drop, etc. if there's odd behaviour it will be clunky
    - making it smooth, working in different browsers, etc.
    - front-end problems can be very specific - sometimes hard to figure out why things happen
    - assume that there should be a dashboard, etc.
    - using UI to generate langchain code is an interesting idea
      - what is the connection?
    - we can tone down the ux and use a questionnaire, etc. want to know what's underneath that ux
    - that makes it tweakable
    - drag and drop, etc. is the most extreme
    - there might still be a visual end-to-end LLM app building approach that isn't so drag and drop (so free) - i.e., more structured
      - to constrain the UI

- TL: demo - has a GUI for setting up a DAG
  - has been peeking at the code to try and understand the code
  - RAGFlow
  - can create a knowledge-base
  - "RAG in a box" - focused on OCR for chunking documents
    - use model - deepdoc on HF for OCR
    - also use another model for tables, etc.
  - can choose an embedding model in the code base - there's code to "plug-in" embedding choices, etc.
  - can choose chunking methods:  books, tables, etc.
    - very visual
    - has some other options
    - RAPTOR:  a new paper for a method of retrieval
  - can add files
  - can do retrieval testing

  - CL: imagine having integrations to slack or discord, website, etc.
    - imagine you can take all medium articles, youtube transcriptions, create a "Chris-bot"
    - if you could embed this on a website, etc. 
    - Chris gets questions about the course, when does it start, etc. - there's a lot of info out there

    - RAG Flow - helps you identify what we've built

    - GL:  this seems to be the extent of front-end to engage in

    - CL:  need to be careful with bots
      - if they say something wrong then company can still be liable

    - Graph feature:  can create an agent - e.g., "helper" (a demo) and you can run it
      - can follow the lines to see what the conversation is
    
    - CL:  do they do evals?  change dbs?  hidden details?
    - CL:  how popular is ragflow?  20k stars
      - project launched in mar / apr 2023
      - hacker news crowd is a tough crowd - great
    - if we do this project:  need to constrain scope to protect ourselves
      - ease-of-use (not for devs) is probably better
        - don't ever build a git or a code-editor - too much criticism
      - 1 click install for users, founders, etc.

  - Inkeep vs RagFlow
    - inkeep might be more enterprise - more of a service
    - ragflow is more DIY - lower-level

- CL:  any time we don't understand something, it might be better to just stop
  - time-box things
  - there are so many things in this space
  - we dont' need to push through dense things that we don't understand
  - always go back to what we do understand
  - how do these things integrate with eval frameworks

  - how do we test over time?
  - e.g., RAGFlow / InKeep - we should try and log in
  - with RAGFlow there are tweaks
  - CL:  would love to see the tweaks and the data snapshoted and sent to an eval framework so that things can then be tested, etc.
  
  
- TL:  graph:  uses a JSON file to develop the graph picture
  - CL:  doesn't understand graph and agent stuff
  - CL:  likes the idea of everytime you make a change, it snapshots
  - if you do have an eval framework, you can go back to your snapshots

  - RAGFlow interface to get into a db - you can select model, vector db, etc. and then snapshot configs and track evals over time
    - this is what LS wants
    - staff can use it, staff can do HITL evals or programmatic feedback

    - can then accumulate data over time and see how it relates to changes to the RAG pipeline

  - what could we use under-the-hood?  should we use LangChain?
    - there is a model router on AWS (Bedrock)
    - LLM - Simon Willison tool to use a command-line to change models
    - LangChain does everything - it brings the kitchen sink when you just need 1 item
    - LS built their own model switcher instead of using the LangChain version

- Promptly
  - CL:  too big
    - like Zapier - it's too much of a platform
    - we would have to slice out a piece of it
    - if we're going to Walmart - just look at purchasing 3 items
    - give a demo of just 2 contained use cases that people would understand
  - the size of the funding - is probably too big


- CL: just keep demoing tools and products
  - there's no wasted time if we're able to share like this
  - understand botpress, hebbia, promptly
  - RagFlow
  - DeepEval, etc.
  - great to know about these ecosystems
  - this is how you explore a brand new area
    - keep exploring

- CL: wants to know about evals
  - what's the best practice in this space?
  - we need to know the context
  - want to have more justifications for why we make particular choices

- CL:  RAG on AWS book
  - if we do this, don't get stuck on fine-tuning
  - it is helpful on general knowledge of LLMs (e.g., LoRA)
  - HITL - RAG, different from RLHF
    - we should be able to disambiguate


