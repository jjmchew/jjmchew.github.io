# Wed Jul 10, 2024

## meeting w/ Chris
- CL:  has a list of vocab words that we should know
  - it's helpful to have that language handy
  - he'll keep phone and notepad handy to look things up later

- CL:  why are we doing demos, why are we logging in?
  - we're looking for a use case - the problem domain
  - specifically the problem and a very specific use case

- CL:  e.g., promptly - a large project
  - perform a specific action end to end
  - story generator was good for that feature-set
  - what do we get at the end

- BB:  research - possible deadend?
  - looked at RIAB and eval framework - looked like this might be too big
  - CL:  we're anthropologists - we're not native AI engineers, yet
    - we should be studying what people do now
    - how do people currently solve this problem?
    - if you can't find that then it's not a problem
  - CL:  thought DeepEval was really good
    - how does that plug into RAG deployments?
    - how do they connect?  This is something to study
    - once we figure this out for DeepEval, we should figure it out for other solutions : what do people do now?
    - capstone is just an exercise in anthropology - what do you do now?

  - CL: the most dangerous answer is you answer something incorrectly
    - e.g., we say flowise doesn't do something and it ends up doing it
    - evals are a huge thing (e.g., at the conference)
      - people are running into that problem
      - the technical bar now could be low, but we need to be sure

    - just building something to connect RAGFlow to DeepEval could be a project
      - but we should make sure that it's a valid pattern before we commit to building it

 
    - basic pipeline:  RAG, connector, DeepEval
      - if that connector connects to more things then it becomes a hub
      - that could also be a thing

    - we can do whatever for the capstone project, but we need "permission" to do it based upon industry practice
      - if we can point at existing things, then it's the best

    - you might just need ask DeepEval:
      - we have a homegrown RAG, how do we connect it to DeepEval?
      - pretend you're making an LS RAG

    - solving the problem is more like being a car mechanic - need to tinker and figure it out
      - ultimately, you need a part and you've got to get it from someone
      - it's not about coming up with it yourself
      - you need to play around more


  - BB:  looked at flowise - do they plug into an eval framework?
    - they have db storage for inputs / outputs
    - doesn't appear that there was an existing solution with a RAG-in-a-box with an eval framework
    - CL:  feels like saying "there's no solution here" is dangerous thing to say
      - we don't want to be there either if there are no solutions



- GL: looked at RIAB + eval
  - How to integrate DeepEval?
    - it's a framework - import it as a test case (like jest)
    - you create a test library
    - what's the infra story?  it's open-source, but if you want a UI and all metrics and interface, you need to use the cloud platform
    - can it be self-hosted?  appears to require cloud-hosting
    - it would go to DeepEval AWS
    - the testing library itself is open-source
      - CL:  did Ben login the other day?
      - BB:  logged into their demo account
    
    - CL:  DeepEval - is a company, wants to make money - that's good
      - CL:  how do we integrate?  it feels very detached
      - how do we get data connected?
      - GL:  it seems separate - testing files are separate from code
      - CL:  where does testing data come from?
      - GL:  code has wrappers around LLMs and can call the model
      - CL:  How would LS use DeepEval to run an eval on our own model?
      - GL:  go into code, import their LLMTestCase utility function
        - their utility function will score it
      - CL:  library is included in app;  when does this get run? (is it like a unit test where the programmer runs it?)
      - BB:  you can upload datasets and have it run through to test them
      - CL:  if I migrate to deepeval, I should already have that data (inputs / outputs) to use to evaluate as a comparison
      - BB:  think it does all types of tests - HITL and automated, etc.
      - GL:  stopped looking at DeepEval after it was clear you couldn't self host

- GL:  Langfuse
  - started in observability and are moving into evals
  - open-core business model
  - CL:  part of langchain?  
  - GL:  no, have YC funding
  - GL: import as a dependency
    - integrate with langchain, llamaindex and vanilla pipelines
    - can import a wrapper around your LLM calls
  - can be self-hosted
  - they have a docker image, to spin it up, need to connect with a PG db with your info
    - it's entirely local
    - created a RAG app in langchain
  - need to import langfuse
  - created a Langchain RAG
  - can look at the different retrievals - can see the traces
  - CL:  debugging is tough:  having traces is really nice
  - GL:  used the medium article since LS website couldn't be scraped
  - CL:  public website is handled by webflow
    - should put cloudfront in front of it to prevent DDoS attacks
  - GL:  also gives costs per token

  - CL:  what's the license on the open-source project?
    - see license file in github repo
    - if we wrap langfuse, we might need to offer a choice
    - if you're not an enterprise user of langfuse, you could use something else
    - it might be worth asking langfuse
    - if we build this, can LS use your project if langfuse is the only project?
      - should we offer options?
    - ask if it can be used in production?
    - ee is "enterprise edition"

  - CL:  where's the eval part?
    - is it just observability?
    - did they just tack on evals since it's hot?

  - CL:  we could add observability in our pipeline
    - we have core rag, observability, and evals
    - langfuse is definitely in the observability box
    - think that eval might be more important
    - if we can add observability later, it would be amazing "bonkers"
      - may need to ask if we can make a wrapper
      - as an MIT expat license, it may be okay?  always best to ask
    - we could potentially just remove the enterprise edition
    - we're just a capstone project, so we won't get funding
    - eval via UI is not available, self-host is not available
      - we would have to use an external eval pipeline
      - Langfuse fetches from traces and then integrates with other packages

    - CL:  the existing interface is definitely NOT a eval interface
      - it's definitely for observability - it's hard to use for eval

      - for LS TA's : want TAs to have a better interface
      - user feedback may not be evals

- GL:  other package:  Dify

- CL:  a danger:  if we build something there will be an all-in-one solution later


- TL:  demo on front-end space - chunking, embedding
  - kind of came up empty-handed
  - CL:  never empty-handed, knowing dead-ends is great
  - TL:  vectorize.io :  learned about it from hacker news post
  - CL:  when you use their platform, if you upload identifying info, be careful
    - not sure how much usage they get - if they see entries coming in, they may look at them
  - TL:  can customize vectorization strategy
    - can pick between vector dbs
    - picked a custom strategy so can pick different embedding models
    - strategies seem to be general best practice, retrievals pulled back, size / overlay, etc.
    - then run experiment
  - seems to follow best practice: it verifies that each chunk should have a sample query or more
  - looked at NDCG:  a mathematical stat that compares what a query result matches against the expected
    - not sure how relevancy is measured
  - CL:  LS ran into a similar problem:
    - weren't sure about chunking strategy
    - consultants said it doesn't matter, they were insistent on evals and tweaking
    - especially since usage is small
    - first thought was - who knows what these knobs do?  great to see that there are some metrics at the end
      - need to see metrics between different configs
    - great to see some base metrics

  - CL:  may be a bit tainted from experience from building a homegrown rag
    - currently not interested in chunking strategies
    - if industry impression is this is a magical incantation and see
    - if we build a framework in this area, concerned that we'll see pushback since people won't think it's a problem

    - you should be able to run all chunking strategies at once and then see the impact
    - CL:  wants HITL to see how things change over time

    - this would be super important if AI tools were being deployed in important things b/c of hallucination, probabalistic answers, etc.
      - it could be used as a supplement to TAs
      - since LS business model is only $200/mo for core

  - CL:  has talked about how much can we afford to use AI?
    - sad part of education - the people who need it the most tend to have the least amount of resources
    - tracing cost (langfuse) is great

    - this problem (chunking) doesn't "hit me" as much
    - chunking could be a front-end strategy

  - GL:  open-source?
    - TL:  no
    - CL:  let's check NDCG and Relevancy metrics - it might be worth seeing how they are calculated

- JC:  demo Agenta and ChainForge
  - feedback:  need to explain the WHY behind features
    - WHY would someone want to do x or y?
    - need to give context, otherwise it's just a bunch of features which don't mean anything


- CL:  unstructured.io
  - we can avoid parsing unstructured data
  - we can just say we'll only take structured data
  - with unstructured data you have to use unstructured.io

  - could look at deepdoc if it's MIT license
  - could also look at unstructured.io


- CL:  look at eval frameworks
  - especially the ones listed under Langfuse integration
  - look at wandb:  they'll be a heavyweight
  - want to look at RAG-in-a-box options
    - find more alternatives
    - ability to upload data


  - could also be just dump your data into database
  - JC:  can we avoid search?
  - CL:  vector dbs may not stick around
    - it should be easy to offload
    - you always rely on something else, e.g., like elasticsearch
    - have been talking about building search in LS
    - don't want to tackle permissions issues
      - need to worry about subscription status, curriculum, deprecated courses, etc.
        - too much to consider
        - the search part isn't hard
    - search is a technical concern we can talk about later when we get there
  - capstone project is flexible
    - we can decide what not to work on
    - we don't need to work on hard technical problems - e.g., unstructured.io
    - or remove that from the scope

  - we don't want to be too annoyingly simple to other products
    - best to know what other people do now and we're beyond the simplest of the simple
    - don't want to be below standard
    - baseline is based upon our research
    - everything we've seen we should be able to do





