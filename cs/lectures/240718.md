# Thurs Jul 18, 2024

## meeting w/ Kevin Coyle and Chris

- Kevin is into GenAI and has his own startup
- has been a data scientist, building emissions models or genAI models
- did ML before genAI

- KC:
  - lives in Honolulu (4 yrs)
  - background:
    - computational linguistics, NLP, liked it and it's having a hey-day now
    - previously data scientist, now ML engineer
  - create on-boarding using genAI into a system to create a training doc and a chat interface
  - attended general assembly way back

- CL:  we're not a bootcamp, but we are a training program
  - we're working on capstone project - working for several weeks after longer term training (a year or more)

  - have done research, and looking for feedback, we've been meeting every day, looking for progress

- GL:  presented deck
  - brief intro: UX background but transitioning into dev

  - slide presentation was about 10 mins with architecture


- KC questions
  - wanted to start high-level
  - first thing running through his mind
  - there is a gap in the market - people who can build and deploy and people who want them
  - he wasn't super clear on who our target audience is
    - technical teams?  kinda?
    - he should be able to fudge his way around if he's technical
  - GL:  SMB with a technical team but not the resources to dedicate people to this feature
    - high configurability and a playground with deployability
    - a technical team could ramp up, but making changes could be painful
    - "developer experience"
  - KC: is this like a vercel or heroku?  abstract some of the parts away
    - GL:  we're not doing anything new, but we're piecing things together that makes people lives easier
  - KC:  AWS is like this - they take open-source and battle-harden it and make it ready
    - create managed services
  
  - KC:  downstream use of RAG - e.g., chatbots
    - if you're selling this to someone writing the cheques, need to lead with the use cases
    - for people on this call, we can think of value, but the ability to talk about benefits to business owners is important
  
  - KC:  for SMBs - but is there a sector you might focus on?
    - GL:  we haven't narrowed down the use-case yet
    - initially we were thinking about technical documentation, but it's still up in the air
    - KC:  getting really tight on who we're talking to and who they're talking to to sign cheques (if they aren't the same person)


  - KC:  slide that talks about AWS - on-prem, etc. slide 15
    - got tripped up:  what is 'on-prem control with automatic deployment from your AWS keys'
    - GL:  clarify that we're spinning up 
    - KC:  "byoc" bring your own cloud
      - you could use ISV marketplace - it already plays with the AWS stack
      - Is this an API service?
      - or is it a GUI that gets hosted on AWS?

    - GL: it's a service that will stand up on its own on AWS, we're helping to automate that process for them
    - KC:  why not host yourself?  multi-tenant thing?


  - KC: a bit more like Rag flow?  or spinning up services on your behalf?
    - GL:  explanation....
    - I was looking for Ragflow

  - KC:  you're bringing an opinionated stack to the RAG process

  - KC:  do I give you your AWS keys
    - GL:  the application will spin up this architecture on your AWS 
    - ... explanation ...
  - KC:  we're spinning up Saas-in-a-box
    - after spinning it up, we'd receive an iframe back
    - rad


  - KC: on functional diagram
    - I see a big box that says "back-end"
    - this serves the chat interface?
    - GL:  all of the parts live on the back-end
      - some explanation, walking through discussion

    - evals will happen for each query every time which could get expensive

  - KC:  1. you have technical people to setting this to
    - 2. also end user is interacting with the chat interface on a website
    - 3. someone may need to figure out what docs to pass over, etc.

    - ingestion thing:  as you ingest docs from whatever person can feed docs into this thing
      - you don't have to go straight to embedding (but maybe you want to)
      - there are strategies where you only take out relevant text for what you want to accomplish
      - e.g., end user only needs to ask questions about as subsection of those docs
      - e.g., SMB sells paper from Ohio, the only thing they want, or that customers care about, are questions about the paper itself
        - ONLY feed that subsection in
      - between ingestion and vector db, we could use an S3
        - when it's time, then have a separate process to embed and do that in batch later
        - may not want to embed 1 by 1
        - be thoughtful about what's going into vectors
        - make sure that what goes in is relevant to the usecase
      - KC:  had great success with PGVector
        - could use it as a vector store with a hotel client
        - they were an AWS shop, pre-domininatly AWS services
        - could use RDS and add PGVector extension
        - nice:  all in AWS, they could store non-vectors, can create normal tables, didn't have to think about spinning up or down different dbs for different use cases
      
    - KC:  back-end and serving
      - need to be the most thoughtful
     - we could use lambdas (could work for serving layer)
      - need to be clever about when someone is interacting with service via front-end, there are clever ways to think about processing that request
        - could have EC2 always up with an endpoint
        - or have a service to manage jobs
        - then have a message bus - could go to kafka and then go to lambda to trigger job to kick off processes
        - those are well-trodden within the AWS eco-sphere
        - with an opinionated stack, you could do it all for the user and they would never have to think about it
      - on the back-end:  that back-end square is like the workhorse of a lot of the application - where ingestion might live
        - uploads document and other stuff could live on the same physical server
        - should be thoughtful about it
        - EKS works pretty well - can deploy with EKS pretty easily
    - ingestion:  embedding models are cheap with openAI, but when users query, that system can really bottleneck performance
      - simple example:  just 10 sentences of all info in db
      - complicated version:  1mm lines all as individual lines
      - when you query the system later from chat to vector db, you need to scan across all of those
        - scanning is cheap (e.g., cosine similarity, etc.)
        - actual storage and embedding is cheap
        - but what gets returned is the BASIS for the users follow-up questions - that's where things get expensive
          - it's a curation exercise to ensure that what comes back is critical
          - how you think about what goes into the vector store is important

  - KC: a re-ranking step is almost always necessary
    - if that re-ranking is within the search or not is up to us
    - also not a terrible idea
  - not just relying on RAG is important
    - a lot of user feedback where they created a semantic search for FAQ, but most people just want keyword search
    - they had to implement both side by side - e.g., Apache Solar and then do semantic search later
    - then do internal ranking
    - it all depends on what the business will upload and what they are expecting
 
  - KC:  uses github for version control
    - knows exactly the words he wants to find, but he just doesn't know what file it's in
    - a lot of time, people may not care about semantic search
    - e.g., in github : exact keyword matching is better

- GL:  we're using llamaindex which should make creation of keyword vs semantic a bit easier

- GL:  other indexes?
  - KC:  what do you mean?  indexes that you use or that you retrieve?
  - GL:  indexes are a part of LlamaIndex (e.g., vectors, keywords, graph, etc.)
    - we might use an LLM-powered query selector
    - KC:  I didn't know that - having various versions of search is important, no matter what tool you're using

- GL:  is it necessary to put things into a separate interface?
  - KC:  we put it all into 1 interface, but served it in 2 ways
  - KC:  likes google : it's like a recommender system, but recommending knowledge or words rather than movies
  - you have candidate generation - from the universe, what subset matches what the user is talking about,  the ranking of that is our internal business logic
  - candidate search:  it could be keyword or semantic - a combo is best
  - how we flip those results around is a secondary task, but a good task to do


- JC:  use of llamaIndex, etc.
- KC:  in the companies he's sold stuff to, they have an architectural review board
  - a group of people who meet to shred the architecture up - they go through the pieces, what are you using, etc.
  - if you use tools that sit on other tools, it gets more complicated (e.g., langchain, llamaindex)
  - now you have to explain those tools and the frameworks
  - it's hard to get things through security, so you have to be clever about why you're using packages
  - that's why they ended up with AWS - used approved tools


  - around software itself
    - llamaindex, langchain - it's bloated software, they do things you need
    - e.g., vector indexing and keyword indexing, and summary, etc.
    - but you may not need that
    - he doesn't see caching, but it may come with the software and expands the footprint and can lead to future problems

  - they just coded on their own, they could point to what worked and what didn't work
    - didn't want to bring in new tools

  - CL:  lots of defense clients - tight on security

- TL: for ingestion - embedding doesn't happen when the document comes in, but best to clean data first
  - how might someone implement that?
  - KC:  easiest way to curate data going in is to think about when you have chunks - there are strategies that look at semantics and topics within your content
    - if chunking is doing it's job well, you could separate each paragraph as it's own ideas
    - you could process with software, it might not be fit
    - semantic chunking
    - langchain could do this out of the box

  - KC:  how do we think about ingesting on the fly vs batch
    - you don't have the need for someone to upload and search right away
    - it sounds static right now
    - if the team implements it, they will identify docs first
    - then, later someone will query the docs


- BB:  use case - if documents are readily available, we're just trying to help fill the void of making that info available
  - is it manageable to a wider scope, or are we better off focusing on just text
  - KC:  it's a business question - it's not just a technical question
    - build it for exactly 10 people, don't talk to technical people
    - just talk to someone who wants to implement it
    - talk to them directly and ask them what you would use it for
    - if I give you this system, what would you use it for
    - that feedback will inform how you focus
    - need to talk to people who are not ourselves - they will tell us things that are outside of our bubble and the tech
    - need to think about how the tool is used


CL:  question:  suppose we're done with the project
  - when we're done with the project how would you use it

  - KC:  the 1 liner:  we created a system where we bring our own cloud and create a rag

  - CL:  in your mind you go to a browser and go to a website?
    - what do you think happens next?
  
  - KC:  sounds like you input keys into web
  - or something like terraform script and plug values into yaml and then hit `project3init`

  - CL:  did you realize this will be open-source?

  - KC:  didn't get that
  
  - CL:  the way you were answering, it felt like a mismatch
    - creating an open-source, we dl a package onto your laptop
    - you don't need to add anything onto the web
    - we may not even have a hosted version

  - KC:  has heard lots of pitches
    - lead with what it looks like now
    - in the consulting world, need to lead with the executive summary
    - plant what's going on for everyone
    
  - CL:  the use case
    - could be like this:  the goal of the project isn't to get users, but want to build something useful and interesting

    - it's not a pure start-up

    - looking at mendable - private, have funding, etc. - since they have funding, that problem must exist
      - we cheat, since that solution is out there, it must be valid
    
    - our use case is similar to mendable, but we need an opensource mendable since you can't ul documents on mendable's infra
      - bring your own infra
      - deploy to your own cloud
      - now you have this internal chatbot

    - IT might not build it, but this open-source tool could spin it up

  - KC:  ....
    - that makes sense
    - if your target is a sales team at a company or a SMB
    - getting tight on who you're talking to (a dev) - if it's open source, it's definitely a developer
    - that's a good lane

- CL:  has feedback on setting up the use case, etc.
  - when talking about existing options, don't go negative on them - just talk about what they do
  - then show how they differ - if you're negative on them, then you create pushback
    - they can get stuck on that
  - then just focus on use-case
    - e.g., minivan vs sports car
    - will naturally constrain itself

  - never want our project to be "all green" - it will make people suspicious
    - we're not trying to get users, we just want this project to make sense
    - you can't pretend it's best of breed - that's a sign that we might not know the space
    - don't claim "best" , just claim a niche

  - show a demo when it's ready - then it would have been obvious it's open-source - we download it
  
  - login to website and giving AWS keys is scary





Kevin@chelle.ai





## meeting w/ Chris afterwards
- CL:  catching with Kevin afterwards, they seemed to know a lot
  - AWS might be a slog - it might bog us down more than the AI stuff

  - doing AWS - if we get really stuck, we can chose a different service and architecture
  - we need to know when to keep slogging vs pick a different path - it depends on AWS skill
  - use resources - everyone's getting bogged down
    - keep posting into the general cohort - there's a good chance that someone's done it already

  - how do you want to spend the next several days?

  - the pitch needs to be a bit tighter, then it'll be fine

- GL:  prototype for demo on Tues @ 10 am
  - CL:  even if we don't demo on AWS - could spin up EC2 then expand out to AWS later

  - that demo:  probably 30 mins
    - feel like we should introduce what RAG is - people won't know
    - need to talk about architecture and tone down the business use case stuff
    - this is how it's different from using chatGPT
    - why aren't we wrapping chatGPT?
  
  - could cut down on the other existing solutions - just talk about mendable (just 1 solution)

  - CL:  might just want to talk about mendable - flowise, ragflow, etc. - it was confusing - a lot to keep in your head

  - e.g., monsoon and flood - it's simple - don't need 5 different solutions.
  - it exists, we're building an open-source version of it
  - there are other solutions, but don't worry about them

  - if people understand mendable, they'll understand our project

  - can focus on the "use your own infrastructure" - this is where we come in

  - now we can demo


- GL:  we don't need to ramp up to everything else, and we're adding some key features that we think that are important

- CL:  acknowledge there are other players
  - but we'll focus on mendable and what they do
  - don't need to be negative about mendable, just focus on our own cloud


- JC:  what time should we send updates?
  - CL:  around 10 am PT

- TL:  we haven't talked about UI, should we do wireframing?
  - it could be a big part of the project

  - CL:  thinks it's the least risk
    - with time we can do it
  - with some of the other stuff, it might not be a time issue
    - it may not be a valid path so we need to determine if those things are viable paths
    - AWS is potentially high-risk

    - UI we should be able to build
    - can use tailwind or bootstrap

  - wireframing is powerful to understand what people want to do
    - but mendable is like a wireframe
  - hence feels UI is less of a risk

  - CL:  thinks biggest risk is still what components can be configured for what
    - architectural questions around ingestion, etc.
    - those are design decisions that you need to justify
    - KC's answer on S3 is great - a "battlefield report" that you didn't have to fight

    - CL:  still thinking about architecture
    - UI just takes energy and effort later

- TL:  re architecture, why not be monolithic and have dbs that are distinct
  - do we need to create a lambda for ingestion?

  - CL:  this is not a question of monolith or not
    - there are other pieces and other services
    - every app is used, and other services (if owned) would be a microservice
      - e.g., stripe, mailchimp, etc. so it would be a bit like a microservice
      - transactional email service, etc.
    - in prod:  even monoliths have a lot of different components
    - in data pipelines (we're building that with search)
      - there's no such thing as monolithic data pipeline, since you need to stop things at every step
      - data flows differently as you process it
      - if you have different steps, it will flow
      - throughput for each process would need to be perfect if you didn't have a db between each step
        - if some steps are longer, slower, take more or less memory, then bottlenecks occur, then you have to distribute and create infra
        - ingest and process and keep pushing to S3 - that's where we get to collect info
        - embedding can slowly pick off S3
        - if S3 increases faster than embedding can process, then eventually something would break - except eventually it will stop (it's bursty)
        - data models require infra and steps to manage the different speeds
        - it's not about monolith or not

      - monolith is talking about code base, and specific use cases
        - e.g., should LS put course enrollment in another service?
        - these terms are fuzzy.  these are "fake words"
        - you could have 2 apps that talk to each other

      - for data, we need the pieces

      - monolith is a code level question

- GL:  if the entire code is on 1 machine
  - ingestion, etc.
  - is it blocking?

  - CL:  YES - it's definitely blocking
  - those components could be infrastructure components
    - nginx and app server could all be on 1 ec2 with db
  - but if you have enough requests, then you could move the db to somewhere else
    - moving the db is an infra decision, not a microservice

  - microservice:  lots of applications that together form 1 workflow

  - CL:  thinks we're still building a monolith with lots of infrastructure
    - just because we have more infra, it doesn't make us a microservice architecture

    - we should split things since they compete for resources on the same machine

    - if you look at other people, you get "permission" 
    - lambda everything could be too granular
    - the answer isn't right or wrong, we could almost just make a choice based on our use case
      - we need to test out our justifications

  - GL:  e.g., we would use a lambda since it's bursty
    - CL:  agreed - don't need to spin up EC2 for that since it will be wasted
      - could be valid for the use case
      - from today's call - all confusion was based around the use case

- BB:  could you take a look at the proposal BB wrote, BB will repost
  - CL will look at it after today


    


